{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.8/site-packages/bytedmetrics/__init__.py:10: UserWarning: bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`\n",
      "  warnings.warn(\"bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`\")\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.70s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(40419, 4096, padding_idx=0)\n",
      "Linear(in_features=4096, out_features=40419, bias=False)\n",
      "Extend Vocab Size:  40419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, LlamaTokenizer\n",
    "\n",
    "origin_model = AutoModelForCausalLM.from_pretrained(\n",
    "    '/mnt/bn/pankeyu/mlx/users/pankeyu/playground/backbones/llama7b-v2',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "origin_tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    '/mnt/bn/pankeyu/mlx/users/pankeyu/playground/backbones/llama7b-v2',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "extend_tokenzier = LlamaTokenizer.from_pretrained(\n",
    "    '/mnt/bn/pankeyu/mlx/users/pankeyu/playground/LLMsTrainer/configs/tokenizer_configs/llama_plus',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "extend_model = AutoModelForCausalLM.from_pretrained(\n",
    "    '/mnt/bn/pankeyu/mlx/users/pankeyu/playground/backbones/llama7b-v2-avg-plus',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "print(extend_model.model.embed_tokens)\n",
    "print(extend_model.lm_head)\n",
    "print('Extend Vocab Size: ', len(extend_tokenzier.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Avg Extend ----------\n",
      "first token embedding:  tensor([ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "        -6.5565e-06,  8.9407e-07], device='cuda:0', dtype=torch.float16)\n",
      "last token embedding:  tensor([ 0.0015,  0.0055,  0.0001,  ...,  0.0054,  0.0036, -0.0061],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "first lm-head embedding tensor([-0.0039,  0.0032, -0.0071,  ...,  0.0053, -0.0082,  0.0070],\n",
      "       device='cuda:6', dtype=torch.float16)\n",
      "last lm-head embedding tensor([-0.0056,  0.0034,  0.0096,  ...,  0.0020,  0.0051, -0.0054],\n",
      "       device='cuda:6', dtype=torch.float16)\n",
      "---------- Random Extend ----------\n",
      "first token embedding:  tensor([ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "        -6.5565e-06,  8.9407e-07], device='cuda:0', dtype=torch.float16)\n",
      "last token embedding:  tensor([-0.0381,  0.0041,  0.0029,  ...,  0.0058, -0.0052,  0.0081],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "first lm-head embedding tensor([-0.0039,  0.0032, -0.0071,  ...,  0.0053, -0.0082,  0.0070],\n",
      "       device='cuda:6', dtype=torch.float16)\n",
      "last lm-head embedding tensor([-0.0122,  0.0348, -0.0177,  ...,  0.0353, -0.0138,  0.0103],\n",
      "       device='cuda:6', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print('-' * 10 + ' Avg Extend ' + '-' * 10)\n",
    "print('first token embedding: ', extend_model.model.embed_tokens.weight.data[0, :])         # 首 token 的 embedding 应该和随机扩词表后的模型值相同\n",
    "print('last token embedding: ', extend_model.model.embed_tokens.weight.data[-1, :])         # 尾 token 的 embedding 应该和随机扩词表后的模型值不同\n",
    "print('first lm-head embedding', extend_model.lm_head.weight.data[0, :])                    # 首 token 的 lm embedding 应该和随机扩词表后的模型值相同\n",
    "print('last lm-head embedding', extend_model.lm_head.weight.data[-1, :])                    # 尾 token 的 lm embedding 应该和随机扩词表后的模型值不同\n",
    "\n",
    "print('-' * 10 + ' Random Extend ' + '-' * 10)\n",
    "print('first token embedding: ', origin_model.model.embed_tokens.weight.data[0, :])\n",
    "print('last token embedding: ', origin_model.model.embed_tokens.weight.data[-1, :])\n",
    "print('first lm-head embedding', origin_model.lm_head.weight.data[0, :])\n",
    "print('last lm-head embedding', origin_model.lm_head.weight.data[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0015,  0.0055,  0.0001,  ...,  0.0054,  0.0036, -0.0061],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([-0.0056,  0.0034,  0.0096,  ...,  0.0020,  0.0051, -0.0054],\n",
      "       device='cuda:6', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_tokens = model.model.embed_tokens                 # Embedding(32000, 4096, padding_idx=0), use `embed_tokens(torch.LongTensor([0, 1]))` to get embedding\n",
    "embed_dim = embed_tokens.embedding_dim                  # 4096\n",
    "lm_head = model.lm_head                                 # Linear(in_features=4096, out_features=32000, bias=False)\n",
    "print(lm_head)\n",
    "print(lm_head.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extend_token_start_index:  32000\n",
      "torch.Size([40419, 4096])\n"
     ]
    }
   ],
   "source": [
    "extend_token_embedding = []\n",
    "origin_tokenizer_tokens = list(origin_tokenizer.get_vocab().keys())\n",
    "extend_tokenizer_tokens = list(extend_tokenzier.get_vocab().keys())\n",
    "assert len(extend_tokenizer_tokens) > len(origin_tokenizer), \\\n",
    "    'Extend vocab size should larger than Origin vocab.'\n",
    "\n",
    "new_embeddings = torch.nn.Embedding(len(extend_tokenizer_tokens), embed_dim)              # allocate new matrix, shape: torch.Size([40419, 4096])\n",
    "new_embeddings.to(embed_tokens.weight.device)\n",
    "\n",
    "new_embeddings.weight.data.normal_(mean=0.0, std=1.0)                                     # initialize weights\n",
    "if new_embeddings.padding_idx is not None:\n",
    "    new_embeddings.weight.data[new_embeddings.padding_idx].zero_()\n",
    "\n",
    "num_tokens_to_copy = min(\n",
    "    len(origin_tokenizer_tokens), \n",
    "    len(extend_tokenizer_tokens)\n",
    ")\n",
    "\n",
    "new_embeddings.weight.data[:num_tokens_to_copy, :] = embed_tokens.weight.data[:num_tokens_to_copy, :]\n",
    "\n",
    "extend_token_start_index = len(origin_tokenizer_tokens)                                   # get the first extend token index\n",
    "print('extend_token_start_index: ', extend_token_start_index)\n",
    "for i in range(extend_token_start_index, len(extend_tokenizer_tokens)):\n",
    "    ext_token = extend_tokenizer_tokens[i]\n",
    "    \n",
    "    # print('Before: ', new_embeddings.weight.data[i, :])\n",
    "\n",
    "    if '<' in ext_token and '>' in ext_token:                                             # don't initial sepcial token like: <pad>, <title>, ...\n",
    "        pass\n",
    "    else:\n",
    "        sub_tokens = origin_tokenizer.encode(ext_token)                                   # e.g. [1, 259, 13]\n",
    "        drop_tokens = [origin_tokenizer.bos_token_id, origin_tokenizer.eos_token_id]      # drop bos/eos token \n",
    "        sub_tokens = [\n",
    "            sub_token for sub_token in sub_tokens if sub_token not in drop_tokens         # e.g. [259, 13]\n",
    "            \n",
    "        ]\n",
    "        sub_tokens_embeddings = embed_tokens(torch.LongTensor(sub_tokens))                # all sub-token embedding in origin tokenizer, shape: (2, 4096)\n",
    "        avg_sub_token_embedding = torch.mean(sub_tokens_embeddings, dim=0)                # averaage sub-token embedding as new extend token embedding, shpae: (4096,)\n",
    "        new_embeddings.weight.data[i, :] = avg_sub_token_embedding                        # update average embedding in new embedding matrix\n",
    "    \n",
    "    # print('Before: ', new_embeddings.weight.data[i, :])\n",
    "\n",
    "print(new_embeddings.weight.size())                                                       # torch.Size([40419, 4096])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40419, 4096])\n"
     ]
    }
   ],
   "source": [
    "old_lm_head_output_features, old_lm_head_input_features = lm_head.weight.size()\n",
    "new_lm_head = torch.nn.Linear(\n",
    "    old_lm_head_input_features, \n",
    "    len(extend_tokenizer_tokens),\n",
    "     bias=False\n",
    ").to(lm_head.weight.device)\n",
    "\n",
    "new_lm_head.weight.data[:num_tokens_to_copy, :] = lm_head.weight.data[:num_tokens_to_copy, :]\n",
    "\n",
    "for i in range(extend_token_start_index, len(extend_tokenizer_tokens)):\n",
    "    ext_token = extend_tokenizer_tokens[i]\n",
    "\n",
    "    # print('Before: ', new_lm_head.weight.data[i, :])\n",
    "\n",
    "    if '<' in ext_token and '>' in ext_token:                                             # don't initial sepcial token like: <pad>, <title>, ...\n",
    "        pass\n",
    "    else:\n",
    "        sub_tokens = origin_tokenizer.encode(ext_token)                                   # e.g. [1, 259, 13]\n",
    "        drop_tokens = [origin_tokenizer.bos_token_id, origin_tokenizer.eos_token_id]      # drop bos/eos token \n",
    "        sub_tokens = [\n",
    "            sub_token for sub_token in sub_tokens if sub_token not in drop_tokens         # e.g. [259, 13]\n",
    "            \n",
    "        ]\n",
    "        sub_lm_embeddings = lm_head.weight.data[sub_tokens, :]                            # all sub-token embedding in origin tokenizer, shape: (2, 4096)\n",
    "        avg_sub_lm_embedding = torch.mean(sub_lm_embeddings, dim=0)                       # averaage sub-toke lm embedding as new extend token embedding, shpae: (4096,)\n",
    "        new_lm_head.weight.data[i, :] = avg_sub_lm_embedding                              # update average embedding in new embedding matrix\n",
    "    \n",
    "    # print('After: ', new_lm_head.weight.data[i, :])\n",
    "\n",
    "# print(new_lm_head.weight.size())                                                        # torch.Size([40419, 4096])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
