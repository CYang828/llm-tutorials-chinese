{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "id": "52ZXbujPsFhz"
   },
   "source": [
    "# Transformers installation\n",
    "! pip install transformers datasets\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFaXCSZCsFh3"
   },
   "source": [
    "# Causal language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXRuXxLlsFh5"
   },
   "source": [
    "有两种类型的语言建模，因果和掩码。本指南阐述了因果语言建模。\n",
    "因果语言模型经常用于文本生成。您可以将这些模型用于创意应用，如选择自己的文本冒险或智能编码助手，如Copilot或CodeParrot。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true,
    "id": "UabouZhPsFh6",
    "outputId": "f330580b-1d9f-4074-fa65-ad87053fd713"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Vpjb1lu0MDk?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Vpjb1lu0MDk?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVi58vaisFh7"
   },
   "source": [
    "因果语言建模预测令牌序列中的下一个令牌，模型只能关注左侧的令牌。这意味着模型无法看到未来的令牌。GPT-2是因果语言模型的一个示例。\n",
    "这个指南将向您展示如何：\n",
    "\n",
    "1. 在ELI5数据集的r/askscience子集上对DistilGPT2进行微调。\n",
    "2. 使用您微调的模型进行推理。\n",
    "在开始之前，请确保已安装所有必要的库：\n",
    "\n",
    "```\n",
    "pip install transformers datasets evaluate\n",
    "```\n",
    "\n",
    "我们鼓励您登录您的Hugging Face帐户，这样您就可以上传和分享您的模型给社区。在提示时，请输入您的令牌以登录："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有两种类型的语言建模，因果和掩码。本指南阐述了因果语言建模。\n",
    "因果语言模型经常用于文本生成。您可以将这些模型用于创意应用，如选择自己的文本冒险或智能编码助手，如Copilot或CodeParrot。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apk3PODrsFh8"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxaUbPelsFh9"
   },
   "source": [
    "## Load ELI5 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUcPy3xusFh-"
   },
   "source": [
    "从🤗数据集库中加载r/askscience子集的ELI5数据集的较小子集。这将为您提供一个机会来进行实验，确保一切正常，然后再花时间在完整数据集上进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOg5Q_CYsFh_"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eli5 = load_dataset(\"eli5\", split=\"train_asks[:5000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUe7MWGrsFh_"
   },
   "source": [
    "使用[train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split)方法将数据集的[train_asks](file:///Users/zhangchunyang/PycharmProjects/nlp-in-action/llm-tutorials-chinese/llm-training/01.pretraining/language-modeling.ipynb#1%2C22-1%2C22)拆分为训练集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5OhZyqpsFiA"
   },
   "outputs": [],
   "source": [
    "eli5 = eli5.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Zb_GmJ8sFiA"
   },
   "source": [
    "然后看一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8gZIMbPsFiA",
    "outputId": "6f767fa6-e2c9-4907-fcd6-24f012215352"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': {'a_id': ['c3d1aib', 'c3d4lya'],\n",
       "  'score': [6, 3],\n",
       "  'text': [\"The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\\n\\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.\",\n",
       "   \"Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?\"]},\n",
       " 'answers_urls': {'url': []},\n",
       " 'document': '',\n",
       " 'q_id': 'nyxfp',\n",
       " 'selftext': '_URL_0_\\n\\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',\n",
       " 'selftext_urls': {'url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg']},\n",
       " 'subreddit': 'askscience',\n",
       " 'title': 'Few questions about this space walk photograph.',\n",
       " 'title_urls': {'url': []}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0P0SmkzusFiB"
   },
   "source": [
    "虽然这看起来很多，但您实际上只对[text](file:///Users/zhangchunyang/PycharmProjects/nlp-in-action/llm-tutorials-chinese/llm-training/01.pretraining/language-modeling.ipynb#14%2C1272-14%2C1272)字段感兴趣。语言建模任务的有趣之处在于您不需要标签（也称为无监督任务），因为下一个词*就是*标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rxsbi3VtsFiB"
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "hide_input": true,
    "id": "a5b7JJl9sFiB",
    "outputId": "05df77a8-4d31-401b-ea94-ecd72a6cf5a7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ma1TrR7gE7I?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ma1TrR7gE7I?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63vQj4qWsFiB"
   },
   "source": [
    "接下来的步骤是加载一个DistilGPT2的分词器来处理[text](file:///Users/zhangchunyang/PycharmProjects/nlp-in-action/llm-tutorials-chinese/llm-training/01.pretraining/language-modeling.ipynb#14%2C1272-14%2C1272)子字段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8UrfE6TsFiB"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuYW4fvGsFiB"
   },
   "source": [
    "您会注意到上面的示例中，[text](file:///Users/zhangchunyang/PycharmProjects/nlp-in-action/llm-tutorials-chinese/llm-training/01.pretraining/language-modeling.ipynb#14%2C1272-14%2C1272)字段实际上是嵌套在[answers](file:///Users/zhangchunyang/PycharmProjects/nlp-in-action/llm-tutorials-chinese/llm-training/01.pretraining/language-modeling.ipynb#1%2C83-1%2C83)内部的。这意味着您需要使用[链接](https://huggingface.co/docs/datasets/process.html#flatten)方法从其嵌套结构中提取[text](file:///Users/zhangchunyang/PycharmProjects/nlp-in-action/llm-tutorials-chinese/llm-training/01.pretraining/language-modeling.ipynb#14%2C1272-14%2C1272)子字段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uV_TdMbXsFiB",
    "outputId": "6639604f-24c8-4908-cb78-6d6a193496b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers.a_id': ['c3d1aib', 'c3d4lya'],\n",
       " 'answers.score': [6, 3],\n",
       " 'answers.text': [\"The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\\n\\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.\",\n",
       "  \"Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?\"],\n",
       " 'answers_urls.url': [],\n",
       " 'document': '',\n",
       " 'q_id': 'nyxfp',\n",
       " 'selftext': '_URL_0_\\n\\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',\n",
       " 'selftext_urls.url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg'],\n",
       " 'subreddit': 'askscience',\n",
       " 'title': 'Few questions about this space walk photograph.',\n",
       " 'title_urls.url': []}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5 = eli5.flatten()\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxBvweZtsFiC"
   },
   "source": [
    "现在，每个子字段都是一个单独的列，如“answer”前缀所示，“text”字段现在是一个列表。不要单独标记每个句子，而是将列表转换为字符串，以便可以联合标记它们。\n",
    "\n",
    "以下是第一个预处理函数，用于连接每个示例的字符串列表并标记结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76Ue_y1OsFiC"
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBfy0_XPsFiC"
   },
   "source": [
    "要将此预处理函数应用于整个数据集，请使用🤗 数据集[map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map)方法您可以通过设置“batched=True”来同时处理数据集的多个元素，并使用“num_proc”增加进程数量，从而加快“map”函数的速度。删除任何不需要的列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNdz4QwIsFiC"
   },
   "outputs": [],
   "source": [
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHOStM3zsFiC"
   },
   "source": [
    "此数据集包含令牌序列，但其中一些令牌序列比模型的最大输入长度长。\n",
    "现在可以使用第二个预处理函数\n",
    "\n",
    "- 连接所有序列\n",
    "- 将连接的序列拆分为由“block_size”定义的较短的块，该块既应短于最大输入长度，又应短到足以容纳GPU RAM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tcBTI8tsFiD"
   },
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvYuG9_SsFiD"
   },
   "source": [
    "将“group_texts”函数应用于整个数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zOx0JR2sFiD"
   },
   "outputs": [],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v0mAa77sFiD"
   },
   "source": [
    "现在使用[DataCollatorForLanguageModeling](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling)创建一个示例批次。在整理过程中，将句子动态填充到批次中的最大长度，而不是将整个数据集填充到最大长度。\n",
    "\n",
    "使用结束序列标记作为填充标记，并设置`mlm=False`。这将使用输入作为标签，向右移动一个元素：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGsiiEorsFiD"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc74pZ8fsFiD"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp4-GBf5sFiE"
   },
   "source": [
    "如果你不熟悉如何使用[Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)进行模型微调，请查看[基础教程](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)！\n",
    "\n",
    "现在你已经准备好开始训练你的模型了！使用[AutoModelForCausalLM](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForCausalLM)加载DistilGPT2："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EH4lrlgsFiE"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdqQoWM-sFiE"
   },
   "source": [
    "到这一步，只剩下三个步骤：\n",
    "\n",
    "1. 在[TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments)中定义你的训练超参数。唯一必需的参数是`output_dir`，它指定了保存模型的位置。通过设置`push_to_hub=True`（你需要登录到Hugging Face才能上传你的模型）将这个模型推送到Hub。\n",
    "2. 将训练参数连同模型、数据集和数据整理器传递给[Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)。\n",
    "3. 调用[train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train)来微调你的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMxt2IRCsFiE"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_eli5_clm-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ln0FuUpLsFiE"
   },
   "source": [
    "一旦训练完成，使用[evaluate()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.evaluate)方法来评估你的模型并获取其困惑度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-VKowUWsFiE",
    "outputId": "31e263dd-2ae4-4d67-a7c4-9341aa293ec7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perplexity: 49.61"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ed9Xl57psFiF"
   },
   "source": [
    "然后使用[push_to_hub()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub)方法将你的模型分享到Hub，这样每个人都可以使用你的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggAjnEIpsFiF"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcAXntI9sFiK"
   },
   "source": [
    "提示\n",
    "\n",
    "如果你想要一个更深入的示例，了解如何为因果语言建模微调模型，请查看相应的\n",
    "[PyTorch 笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)\n",
    "或 [TensorFlow 笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjpM34OnsFiK"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWTTlXUFsFiL"
   },
   "source": [
    "太好了，现在你已经微调了一个模型，你可以用它来进行推理！\n",
    "\n",
    "想出一个你想要从中生成文本的提示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkL__DXssFiL"
   },
   "outputs": [],
   "source": [
    "prompt = \"Somatic hypermutation allows the immune system to\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCs7-4b7sFiL"
   },
   "source": [
    "尝试使用你微调的模型进行推理的最简单方法是在[pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline)中使用它。用你的模型实例化一个文本生成的`pipeline`，并将你的文本传递给它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aByVLcxhsFiL",
    "outputId": "2df8b7a4-258c-4cb0-e813-fec1b381a95f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Somatic hypermutation allows the immune system to be able to effectively reverse the damage caused by an infection.\\n\\n\\nThe damage caused by an infection is caused by the immune system's ability to perform its own self-correcting tasks.\"}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"my_awesome_eli5_clm-model\")\n",
    "generator(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1J14vGGQsFiL"
   },
   "source": [
    "对文本进行分词并将`input_ids`作为PyTorch张量返回："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxiIE3dbsFiL"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_eli5_clm-model\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3N8x0bsPsFiM"
   },
   "source": [
    "使用[generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate)方法来生成文本。\n",
    "想要了解更多关于不同文本生成策略和控制生成的参数的详细信息，请查看[文本生成策略](https://huggingface.co/docs/transformers/main/en/tasks/../generation_strategies)页面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfoGfZifsFiM"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"my_awesome_eli5_clm-model\")\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tM78HtFNsFiM"
   },
   "source": [
    "将生成的令牌ID解码回文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99B4O1XssFiM",
    "outputId": "622c669b-2c75-416a-8cfe-62b6b7e8462c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Somatic hypermutation allows the immune system to react to drugs with the ability to adapt to a different environmental situation. In other words, a system of 'hypermutation' can help the immune system to adapt to a different environmental situation or in some cases even a single life. In contrast, researchers at the University of Massachusetts-Boston have found that 'hypermutation' is much stronger in mice than in humans but can be found in humans, and that it's not completely unknown to the immune system. A study on how the immune system\"]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
