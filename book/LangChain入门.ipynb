{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fv8-liKls3tj"
   },
   "source": [
    "# 介绍\n",
    "![image.png](https://aimaksen.rarelimiting.com/langchain/l1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64MzIT8YuIgn"
   },
   "source": [
    "# 基础功能\n",
    "\n",
    "![image.png](https://aimaksen.rarelimiting.com/langchain/l2.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zx2d9ciBuYLz"
   },
   "source": [
    "# 必会概念\n",
    "\n",
    "![image.png](https://aimaksen.rarelimiting.com/langchain/l3.jpg)\n",
    "![image.png](https://aimaksen.rarelimiting.com/langchain/l4.jpg)\n",
    "![image.png](https://aimaksen.rarelimiting.com/langchain/l5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzAxkbKIurUA"
   },
   "source": [
    "# 实战\n",
    "\n",
    "通过上面的必备概念大家应该已经可以对 LangChain 有了一定的了解，但是可能还有有些懵。\n",
    "\n",
    "这都是小问题，我相信看完后面的实战，你们就会彻底的理解上面的内容，并且能感受到这个库的真正强大之处。\n",
    "\n",
    "因为我们 OpenAI API 进阶，所以我们后面的范例使用的 LLM 都是以Open AI 为例，后面大家可以根据自己任务的需要换成自己需要的 LLM 模型即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ao78SDEp1Xbf"
   },
   "source": [
    "# 第一个案例：使用 LangChain 完成一次问答\n",
    "第一个案例，我们就来个最简单的，用 LangChain 加载 OpenAI 的模型，并且完成一次问答。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZgYO1xijejp"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yH_o5pZM0pUj",
    "outputId": "666761ff-d380-4bab-df0d-0ba60bcc57fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.266)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.22)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kk01CyyA1GwX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18J8eoDjvcgR"
   },
   "source": [
    "然后，我们进行导入和执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "TSeOZmGJ1aL_",
    "outputId": "d392ac4b-dff3-4fd5-d2f8-2c59e5df55dd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\n人工智能是当今科技发展的一个重要组成部分，它改变了人们的工作方式，让许多计算任务变得更加自动化和高效。人工智能技术已经在许多领域取得了巨大的成功，让我们的生活变得更加便捷，让企业提高了效率。总体而言，人工智能是一个十分有前景的技术，它可以改善人们的生活，改变整个社会。'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", max_tokens=1024)\n",
    "llm(\"怎么评价人工智能\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn5aYtapvhZ9"
   },
   "source": [
    "这时，我们就可以看到他给我们的返回结果了，怎么样，是不是很简单。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0-Zs6KZ6Dgi"
   },
   "source": [
    "# 第二个案例：通过 Google 搜索并返回答案\n",
    "\n",
    "接下来，我们就来搞点有意思的。我们来让我们的 OpenAI api 联网搜索，并返回答案给我们。\n",
    "\n",
    "这里我们需要借助 Serpapi 来进行实现，Serpapi 提供了 google 搜索的 api 接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z23RBYDKvAxM",
    "outputId": "245272ae-ed7e-4744-872e-d0788bde5176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-search-results in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9onJzdWEv4TL"
   },
   "source": [
    "首先需要我们到 Serpapi 官网上注册一个用户，https://serpapi.com/ 并复制他给我们生成 api key。\n",
    "\n",
    "然后我们需要像上面的 openai api key 一样设置到环境变量里面去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3VCMBux6Iuq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = 'c18d892ffa40ba1184fe4205ebce3bedfa76f8cf9f0c6b0dafd74e56b23f1ec1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shy7V8GcwBbl"
   },
   "source": [
    "然后，开始编写我的代码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "GfyZF-aW6RJa",
    "outputId": "2ef93da4-7b38-4b47-8d3d-7962160afdc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use a search engine to find out the date and then use it to search for historical events.\n",
      "Action: Search\n",
      "Action Input: What is the date today?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe date today is Tuesday, August 15, 2023. What is the date today in numbers? Depending where you reside and who's asking the date in numbers can be shown ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should search for historical events that happened on this date.\n",
      "Action: Search\n",
      "Action Input: Historical events on August 15\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mhttps://www.onthisday.com/events/august/15\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Today is Tuesday, August 15, 2023. On this day in history, the following events have taken place: in 1812, the Battle of Fort Dearborn; in 1877, the first telephone exchange opened in New Haven, Connecticut; in 1914, the Panama Canal opened; in 1945, Japan accepted the terms of the Potsdam Declaration and agreed to surrender in World War II; in 1969, the Woodstock Music and Art Fair began in Bethel, New York; and in 1998, the United States embassy bombings in Dar es Salaam, Tanzania, and Nairobi, Kenya, took place.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Today is Tuesday, August 15, 2023. On this day in history, the following events have taken place: in 1812, the Battle of Fort Dearborn; in 1877, the first telephone exchange opened in New Haven, Connecticut; in 1914, the Panama Canal opened; in 1945, Japan accepted the terms of the Potsdam Declaration and agreed to surrender in World War II; in 1969, the Woodstock Music and Art Fair began in Bethel, New York; and in 1998, the United States embassy bombings in Dar es Salaam, Tanzania, and Nairobi, Kenya, took place.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "# 加载 OpenAI 模型\n",
    "llm = OpenAI(temperature=0,max_tokens=2048)\n",
    "\n",
    " # 加载 serpapi 工具\n",
    "# tools = load_tools([\"serpapi\"])\n",
    "\n",
    "# 如果搜索完想再计算一下可以这么写\n",
    "# tools = load_tools(['serpapi', 'llm-math'], llm=llm)\n",
    "\n",
    "# 如果搜索完想再让他再用python的print做点简单的计算，可以这样写\n",
    "tools=load_tools([\"serpapi\", \"python_repl\"])\n",
    "\n",
    "# 工具加载后都需要初始化，verbose 参数为 True，会打印全部的执行详情\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# 运行 agent\n",
    "agent.run(\"What's the date today? What great events have taken place today in history?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoLMIY4zwmFV"
   },
   "source": [
    "我们可以看到，他正确的返回了日期（有时差），并且返回了历史上的今天。\n",
    "\n",
    "在 chain 和 agent 对象上都会有 verbose 这个参数，这个是个非常有用的参数，开启他后我们可以看到完整的 chain 执行过程。\n",
    "\n",
    "可以在上面返回的结果看到，他将我们的问题拆分成了几个步骤，然后一步一步得到最终的答案。\n",
    "\n",
    "关于agent type 几个选项的含义（理解不了也不会影响下面的学习，用多了自然理解了）：\n",
    "\n",
    "- zero-shot-react-description: 根据工具的描述和请求内容的来决定使用哪个工具（最常用）\n",
    "- react-docstore: 使用 ReAct 框架和 docstore 交互, 使用Search 和Lookup 工具, 前者用来搜, 后者寻找term, 举例: Wipipedia 工具\n",
    "- self-ask-with-search 此代理只使用一个工具: Intermediate Answer, 它会为问题寻找事实答案(指的非 gpt 生成的答案, 而是在网络中,文本中已存在的), 如 Google search API 工具\n",
    "- conversational-react-description: 为会话设置而设计的代理, 它的prompt会被设计的具有会话性, 且还是会使用 ReAct 框架来决定使用来个工具, 并且将过往的会话交互存入内存\n",
    "\n",
    "> reAct 介绍可以看这个：https://arxiv.org/pdf/2210.03629.pdf\n",
    ">\n",
    "> LLM 的 ReAct 模式的 Python 实现: https://til.simonwillison.net/llms/python-react-pattern\n",
    ">\n",
    "> agent type 官方解释：\n",
    ">\n",
    "> https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html?highlight=zero-shot-react-description\n",
    "\n",
    "> 有一点要说明的是，这个 `serpapi` 貌似对中文不是很友好，所以提问的 prompt 建议使用英文。\n",
    "\n",
    "当然，官方已经写好了 `ChatGPT Plugins` 的 agent，未来 chatgpt 能用啥插件，我们在 api 里面也能用插件，想想都美滋滋。\n",
    "\n",
    "不过目前只能使用不用授权的插件，期待未来官方解决这个。\n",
    "\n",
    "感兴趣的可以看这个文档：https://python.langchain.com/en/latest/modules/agents/tools/examples/chatgpt_plugins.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1nWOkGvCStj"
   },
   "source": [
    "# 第三个案例：对超长文本进行总结\n",
    "\n",
    "假如我们想要用 openai api 对一个段文本进行总结，我们通常的做法就是直接发给 api 让他总结。但是如果文本超过了 api 最大的 token 限制就会报错。\n",
    "\n",
    "这时，我们一般会进行对文章进行分段，比如通过 tiktoken 计算并分割，然后将各段发送给 api 进行总结，最后将各段的总结再进行一个全部的总结。\n",
    "\n",
    "如果，你用是 LangChain，他很好的帮我们处理了这个过程，使得我们编写代码变的非常简单。\n",
    "\n",
    "废话不多说，直接上代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttfWa2AAeDfl"
   },
   "outputs": [],
   "source": [
    "!pip install unstructured\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v-Uo6ByohkpR",
    "outputId": "70710d89-fed9-4fab-f095-5b5d6478e8af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8WriJqBU6RDa",
    "outputId": "454b1663-34da-4d76-b200-e36612933d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents:1\n",
      "documents:319\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"声明：本书为爱奇电子书(www.i7wu.cn)的用户上传至其在本站的存储空间，本站只提供TXT全集电子书存储服务以及免费下载服务，以下作品内容之版权与本站无任何关系。\n",
      "\n",
      "---------------------------用户上传之内容开始--------------------------------\n",
      "\n",
      "《地藏心经》\n",
      "\n",
      "作者：铸剑师无名\n",
      "\n",
      "正文\n",
      "\n",
      "第一第十五章 天下势，渡江（一）\n",
      "\n",
      "“渝州陆家？！”\n",
      "\n",
      "虽然原本的那个秦逸，每日只知道苦读诗书，从未与商贾们打过交道，但是渝州陆家的名声，他还是知道。\n",
      "\n",
      "陆家三代为官，官至两江总督，五代经商，百年经营，家私何止千万，直至今朝，俨然已是江南一等士族大户。渝州陆氏以皮货起家，乃是西北之地数得上号的商户，西北之地所产的皮货，有三成经他们之手卖往江南。\n",
      "\n",
      "若只是如此，陆氏也不过是一头肥硕的羔羊，只待他人宰杀。\n",
      "\n",
      "陆氏三代家主都极具雄韬伟略，以千金买官，以万金开路，更是在蛮夷南侵之时，倾尽家资招兵买马，拒十万蛮夷铁骑于侯关外，短短三年间，便一手扶持起了都护大将军——苏和，抗夷大将军——邓昌。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "本书是爱奇电子书(www.i7wu.cn)用户上传的《地藏心经》，作者为铸剑师无名。第一第十五章介绍了渝州陆家的家族历史，以及他们家族的财富和官职。三代家主极具雄韬伟略，拥有开路招兵的能力，扶持了苏和和邓昌的军队，为抗击蛮夷侵略作出了贡献。\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "以姻亲握住兵权后，陆氏子弟一路仕途平坦，百年来，人才辈出，更有陆云，陆羽等良将贤才。\n",
      "\n",
      "而今，已是雄踞渝、豫两地的世家阀门，这江南数万水军，便是掌握在这一代的陆家族长手中。\n",
      "\n",
      "朝廷无权，皇帝无兵，短短十年，南朝便形同虚设，各地封疆大使，世家阀门手握重兵，除了京都三省还在南朝皇族手中，其他俨然已经分地而治。\n",
      "\n",
      "西北，邓、李、苏、何、公孙五家世家阀门割据一方，联手共抗蛮夷合并后的金国。\n",
      "\n",
      "南方，陆、熊、刘、郑四家百年士族据守江南，与中山国相持已然数十载。\n",
      "\n",
      "东方，京都三省雄兵三十万，黑甲铁骑八千，时刻防范着秦国有所异动。（备注：黑甲铁骑配备长枪，马刀，黑铁重甲，所乘骑的乃是西域宛马，是南朝立国时便赫赫有名的百战铁骑。曾以八千黑甲铁骑破中山国十万雄兵而名动天下。）\n",
      "\n",
      "这些，便是张狂融合完原本那个‘秦逸’的记忆，而整理出的天下大势。\n",
      "\n",
      "**************************************************************************\n",
      "\n",
      "“少爷。这船都被陆家车行的人包下了。”\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "本书是爱奇电子书(www.i7wu.cn)用户上传的《地藏心经》，作者为铸剑师无名。第一第十五章介绍了渝州陆家的家族历史，以及他们家族的财富和官职。三代家主极具雄韬伟略，拥有开路招兵的能力，扶持了苏和和邓昌的军队，为抗击蛮夷侵略作出了贡献。以姻亲握住兵权后，陆氏子弟一路仕途平坦，百年来，人才辈出，更有陆云，陆羽等良将贤才。而今，已是雄踞渝、豫两地的世家阀门，这江南数万水军，便是掌握在这一代的陆家族长手中，更甚至已经掌控了朝廷无权，皇帝无兵的重兵，以及西北、南方、东方的大势，甚至还掌握了江苏渝州的船只。\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "不过一会儿，秦汉便略显沮丧地走了回来。渝州陆家势大，而今就连附属下面的陆家车行，身份也是水涨船高。自从秦逸父亲病逝后，秦家家道中落，与陆家比不得，况且此地也并非西北所属，秦家纵然还有些人脉，却也用不上。\n",
      "\n",
      "所以，为了避免麻烦，他也没敢去与陆家争船。\n",
      "\n",
      "“嗯。”秦逸默然，脸色平静，对着秦汉点点头，也未多说些什么。虽然他心中也想早点赶往渝州，在年关前，布置些家业，好早些安定下来。“我知道了。”\n",
      "\n",
      "“敢问公子贵姓？”\n",
      "\n",
      "这时，秦逸身旁的中年商人，突然出口问道。原来他见秦逸，wｗ W.l6Ｋ .cN面容俊逸，又是一身锦衣华服，虽然风尘仆仆，但是谈吐举止中，无一不带着士族风范，不由得起了巴结之心。\n",
      "\n",
      "南朝商人地位虽然要略高于前朝列代，但是依旧排在最后。“士农工商”，商人自古就有着“不劳而获”之名。\n",
      "\n",
      "“姓秦。”秦逸面色淡然，转头看了中年商人一眼，出声道。\n",
      "\n",
      "他来于后世，对商人并无轻视之意，所以也没有摆什么士族的架子。\n",
      "\n",
      "中年商人闻言微微一愣，随即动容，隐隐带着喜悦，他躬腰低头，对着秦逸恭恭敬敬地行了一个大礼，而后出声询问道：“敢问可是晋中秦家？！”\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "本书是爱奇电子书(www.i7wu.cn)用户上传的《地藏心经》，作者为铸剑师无名。第一第十五章介绍了渝州陆家的家族历史，以及他们家族的财富和官职。三代家主极具雄韬伟略，拥有开路招兵的能力，扶持了苏和和邓昌的军队，为抗击蛮夷侵略作出了贡献。以姻亲握住兵权后，陆氏子弟一路仕途平坦，百年来，人才辈出，更有陆云，陆羽等良将贤才。而今，已是雄踞渝、豫两地的世家阀门，这江南数万水军，便是掌握在这一代的陆家族长手中，更甚至已经掌握了朝廷无权，皇帝无兵的重兵，以及西北、南方、东方的大势，甚至还掌握了江苏渝州的船只，由此可见陆家的实力之强大。秦逸虽然出身贫寒，但是他的名字已经被陆家车行所熟悉，他也没敢去与陆家争船，而中年商人更是对秦逸的名字十分熟悉，可见此渝州陆家的福相当不凡。\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "“正是！”说话的确是秦汉，秦家在西北之地声名远播，善名百里，虽然手中无兵无权，但是在西北士族中还是举足轻重，俨然已成精神领袖。\n",
      "\n",
      "“敢问，可是秦逸公子？！”中年商人对着秦逸又是一个大礼，声音颇为颤抖地说道。此番回程，他便听说了秦家少爷要前往渝州，却想不到自己居然正好遇上！\n",
      "\n",
      "“五代行善，何其不易！夫天下之人，独晋中秦家也！”……\n",
      "\n",
      "秦家善名，至今已然百年有余。\n",
      "\n",
      "“嗯。”秦逸点头，并未多说。一路行来，他已经陆续感受到了秦家在这个世界上的声望。\n",
      "\n",
      "一世行善容易，但是五代行善，中原数千年来，独此一家。就连数十年前，蛮夷赫连氏族入侵中原，都刻意避开了晋中秦家。在草原蛮族的教义中，屠戮真正的善人，会被狼神抛弃，灵魂永世不得安息。\n",
      "\n",
      "************************************************************************\n",
      "\n",
      "就在秦逸准备寻一处清净地，安安静静的等待陆家车行的人先走时，远处，一团人簇拥着一个青衫老者往这边走来。而为首的，正是昨日在路上遇到的那个满脸扎须的壮年汉子。\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "本书是爱奇电子书(www.i7wu.cn)用户上传的《地藏心经》，作者为铸剑师无名。第一第十五章介绍了渝州陆家的家族历史，以及他们家族的财富和官职。三代家主极具雄韬伟略，拥有开路招兵的能力，扶持了苏和和邓昌的军队，为抗击蛮夷侵略作出了贡献。以姻亲握住兵权后，陆氏子弟一路仕途平坦，百年来，人才辈出，更有陆云，陆羽等良将贤才，已是雄踞渝、豫两地的世家阀门，掌握重兵、大势以及江苏渝州的船只，威望遍布中原，秦家五代行善，至今已然百年有余，甚至蛮夷入侵中原时也避开了他们。秦逸出身贫寒，但是他的名字已经被陆家车行所熟悉，此渝州陆家的福相当不凡。\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "“那便是陆家车行的管事。”一旁的中年商人适时的报出了那位青衫老者的身份。\n",
      "\n",
      "“陆氏车行？管事？”\n",
      "\n",
      "秦逸眉头一挑，不由得心头一动。若是等到陆家车行货物运完，这一来一去，天怕是已经摸黑了，想来渡江只能等到明晚。既然面前，就是陆家车行的管事，何不找他试试，看看能不能一并登船渡江。\n",
      "\n",
      "想到这，秦逸略微整了整衣衫，脸上挂着一副淡定的笑容，迎了上去。\n",
      "\n",
      "“长者有礼了！”秦逸走到人群前，对着为首的青衫老者微微一拱手，行礼道。\n",
      "\n",
      "“你他……”眼前突然多出了一个人来，宋老虎不由一愣，他心底正火着呢~。对于这个突然蹦出来的拦路者，他可是一点好感都没有。可是待他看清来人的模样，又不由微微一愣神。面容俊逸，星目剑眉，身形略显消瘦，一身褐色长衫，虽然沾染风尘，可是也遮不住那一骨子的儒雅正气！\n",
      "\n",
      "“是个读书人。”宋老虎心中不由的想到。他与其他的武林中人不同，他这个人佩服读书人，当然，是那种一身正气的读书人。\n",
      "\n",
      "所以，他在看清楚秦逸的模样后，又不由的将最后那个“妈”字，生生地咽了下去。\n",
      "\n",
      "青衫老者闻言一愣，待他抬头看清秦逸的模样，觉察出他身上那一股子士族风范后，这才拱手回礼道：“公子何事？”\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\n本书是爱奇电子书(www.i7wu.cn)用户上传的《地藏心经》，作者为铸剑师无名。第一第十五章介绍了渝州陆家的家族历史，以及他们家族的财富和官职。三代家主极具雄韬伟略，拥有开路招兵的能力，扶持了苏和和邓昌的军队，为抗击蛮夷侵略作出了贡献。以姻亲握住兵权后，陆氏子弟一路仕途平坦，百年来，人才辈出，更有陆云，陆羽等良将贤才，已是雄踞渝、豫两地的世家阀门，掌握重兵、大势以及江苏渝州的船只，威望遍布中原，秦家五代行善，至今已然百年有余，甚至蛮夷入侵中原时也避开了他们。秦逸出身贫寒，但是他的名字已经被陆家车行所熟悉，此渝州陆家的福相当不凡，在宋老虎的眼中，被认定为一位拥有正气的读书人。'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import OpenAI\n",
    "\n",
    "# 导入文本\n",
    "loader = UnstructuredFileLoader(\"/content/sample_data/data/lg_test.txt\")\n",
    "# 将文本转成 Document 对象\n",
    "document = loader.load()\n",
    "print(f'documents:{len(document)}')\n",
    "\n",
    "# 初始化文本分割器\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "\n",
    "# 切分文本\n",
    "split_documents = text_splitter.split_documents(document)\n",
    "print(f'documents:{len(split_documents)}')\n",
    "\n",
    "# 加载 llm 模型\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", max_tokens=1500)\n",
    "\n",
    "# 创建总结链\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\", verbose=True)\n",
    "\n",
    "# 执行总结链，（为了快速演示，只总结前5段）\n",
    "chain.run(split_documents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-8l1wBIzSuf"
   },
   "source": [
    "首先我们对切割前和切割后的 document 个数进行了打印，我们可以看到，切割前就是只有整篇的一个 document，切割完成后，会把上面一个 document 切成 317 个 document。\n",
    "\n",
    "![image-20230405162631460](https://raw.githubusercontent.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide/main/doc/image-20230405162631460.png)\n",
    "\n",
    "最终输出了对前 5 个 document 的总结。\n",
    "\n",
    "![image-20230405162937249](https://raw.githubusercontent.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide/main/doc/image-20230405162937249.png)\n",
    "\n",
    "这里有几个参数需要注意：\n",
    "\n",
    "**文本分割器的 `chunk_overlap` 参数**\n",
    "\n",
    "这个是指切割后的每个 document 里包含几个上一个 document 结尾的内容，主要作用是为了增加每个 document 的上下文关联。比如，`chunk_overlap=0`时， 第一个 document 为 aaaaaa，第二个为 bbbbbb；当 `chunk_overlap=2` 时，第一个 document 为 aaaaaa，第二个为 aabbbbbb。\n",
    "\n",
    "不过，这个也不是绝对的，要看所使用的那个文本分割模型内部的具体算法。\n",
    "\n",
    "> 文本分割器可以参考这个文档：https://python.langchain.com/en/latest/modules/indexes/text_splitters.html\n",
    "\n",
    "**chain 的 `chain_type` 参数**\n",
    "\n",
    "这个参数主要控制了将 document 传递给 llm 模型的方式，一共有 4 种方式：\n",
    "\n",
    "`stuff`: 这种最简单粗暴，会把所有的 document 一次全部传给 llm 模型进行总结。如果document很多的话，势必会报超出最大 token 限制的错，所以总结文本的时候一般不会选中这个。\n",
    "\n",
    "`map_reduce`: 这个方式会先将每个 document 进行总结，最后将所有 document 总结出的结果再进行一次总结。\n",
    "\n",
    "![image-20230405165752743](https://raw.githubusercontent.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide/main/doc/image-20230405165752743.png)\n",
    "\n",
    "`refine`: 这种方式会先总结第一个 document，然后在将第一个 document 总结出的内容和第二个 document 一起发给 llm 模型在进行总结，以此类推。这种方式的好处就是在总结后一个 document 的时候，会带着前一个的 document 进行总结，给需要总结的 document 添加了上下文，增加了总结内容的连贯性。\n",
    "\n",
    "![image-20230405170617383](https://raw.githubusercontent.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide/main/doc/image-20230405170617383.png)\n",
    "\n",
    "`map_rerank`: 这种一般不会用在总结的 chain 上，而是会用在问答的 chain 上，他其实是一种搜索答案的匹配方式。首先你要给出一个问题，他会根据问题给每个 document 计算一个这个 document 能回答这个问题的概率分数，然后找到分数最高的那个 document ，在通过把这个 document 转化为问题的 prompt 的一部分（问题+document）发送给 llm 模型，最后 llm 模型返回具体答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NySCEE0-tKw-"
   },
   "source": [
    "# 第四个案例：构建本地知识库问答机器人\n",
    "\n",
    "在这个例子中，我们会介绍如何从我们本地读取多个文档构建知识库，并且使用 Openai API 在知识库中进行搜索并给出答案。\n",
    "\n",
    "这个是个很有用的教程，比如可以很方便的做一个可以介绍公司业务的机器人，或是介绍一个产品的机器人。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGoTRXA5tUbW",
    "outputId": "58daeb1a-a217-4b63-f6c7-0ea52802817a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.text_splitter:Created a chunk of size 104, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 101, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 128, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 116, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 109, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 109, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 113, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 106, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 143, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 111, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 214, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 102, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 106, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 119, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 107, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 108, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 107, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 103, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 102, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 108, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 129, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 112, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 128, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 128, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 110, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 129, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 110, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 138, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 119, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 126, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 124, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 128, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 127, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 110, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 111, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 109, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 111, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 116, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 115, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 107, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 121, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 170, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 109, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 118, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 103, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 116, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 114, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 139, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 112, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 130, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 104, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 114, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 113, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 115, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 115, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 114, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 106, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 138, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 115, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 112, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 104, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 114, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 124, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 153, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 103, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 122, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 114, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 145, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 137, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 152, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 135, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 116, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 114, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 107, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 114, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 183, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 114, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 102, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 113, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 114, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 124, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 112, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 115, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 118, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 126, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 113, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 128, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 128, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 114, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 118, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 101, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 102, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 102, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 156, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 126, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 102, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 146, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 111, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 102, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 124, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 107, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 101, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 132, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 150, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 112, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 111, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 101, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 106, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 106, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 139, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 113, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 111, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 152, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 116, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 114, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 149, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 135, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 101, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 104, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 101, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 122, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 125, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 115, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 123, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 121, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 118, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 150, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 110, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 103, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 104, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 141, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 121, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 124, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 126, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 121, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 117, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 124, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 125, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 103, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 124, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 118, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 135, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 118, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 112, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 110, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 109, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 124, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 145, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 163, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 115, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 104, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 113, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 144, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 111, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 125, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 115, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 111, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 138, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 123, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 112, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 143, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 104, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 141, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 102, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 107, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 157, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 150, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 108, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 129, which is longer than the specified 100\n",
      "WARNING:langchain.text_splitter:Created a chunk of size 125, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '科大讯飞今年第一季度收入是多少？', 'result': \" I don't know.\", 'source_documents': [Document(page_content='第一第二章 冬日寒，何须风度\\n\\n“下雪了！”\\n\\n清晨，阳光，微冷。\\n\\n秦逸穿好棉袄，推开格子窗，迎面扑来的，便是一股子清新寒意。', metadata={'source': '/content/sample_data/data/lg_test.txt'}), Document(page_content='第一第一章 浮云若兮\\n\\n“我还没死？！……”\\n\\n秦逸艰难地坐了起来，双手轻柔且有节奏地揉着太阳穴，此刻他脑中还有些昏昏沉沉，眼前的景物也颇为模糊，便如添上几分幻影一般。', metadata={'source': '/content/sample_data/data/lg_test.txt'}), Document(page_content='……………………\\n\\n十万功德！自己将开启第一个“六神通”，同时，也将面对第一道天劫！！……\\n\\n第一第五章 静夜思，独芳华……\\n\\n星月辉耀，烛光点缀。', metadata={'source': '/content/sample_data/data/lg_test.txt'}), Document(page_content='天空的雷鸣声，越来越大，黑云也压得更低了，一抬头，便能看到头顶闪烁的电光……\\n\\n“小姐！！？他怎么了？！”\\n\\n身后传来小桃花的惊呼声，惊呼中，隐隐带着一丝哭腔……', metadata={'source': '/content/sample_data/data/lg_test.txt'})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 加载文件夹中的所有txt类型的文件\n",
    "loader = DirectoryLoader('/content/sample_data/data/', glob='**/*.txt')\n",
    "# 将数据转成 document 对象，每个文件会作为一个 document\n",
    "documents = loader.load()\n",
    "\n",
    "# 初始化加载器\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "# 切割加载的 document\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 初始化 openai 的 embeddings 对象\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# 将 document 通过 openai 的 embeddings 对象计算 embedding 向量信息并临时存入 Chroma 向量数据库，用于后续匹配查询\n",
    "docsearch = Chroma.from_documents(split_docs, embeddings)\n",
    "\n",
    "# 创建问答对象\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever(), return_source_documents=True)\n",
    "# 进行问答\n",
    "result = qa({\"query\": \"科大讯飞今年第一季度收入是多少？\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HVB9Xu82NB8"
   },
   "source": [
    "我们可以通过结果看到，他成功的从我们的给到的数据中获取了正确的答案。\n",
    "\n",
    "> 关于 Openai embeddings 详细资料可以参看这个连接: https://platform.openai.com/docs/guides/embeddings\n",
    "\n",
    "### 构建向量索引数据库\n",
    "\n",
    "我们上个案例里面有一步是将 document 信息转换成向量信息和embeddings的信息并临时存入 Chroma 数据库。\n",
    "\n",
    "因为是临时存入，所以当我们上面的代码执行完成后，上面的向量化后的数据将会丢失。如果想下次使用，那么就还需要再计算一次embeddings，这肯定不是我们想要的。\n",
    "\n",
    "那么，这个案例我们就来通过 Chroma 和 Pinecone 这两个数据库来讲一下如何做向量数据持久化。\n",
    "\n",
    "> 因为 LangChain 支持的数据库有很多，所以这里就介绍两个用的比较多的，更多的可以参看文档:https://python.langchain.com/en/latest/modules/indexes/vectorstores/getting\\_started.html\n",
    "\n",
    "**Chroma**\n",
    "\n",
    "chroma 是个本地的向量数据库，他提供的一个 `persist_directory` 来设置持久化目录进行持久化。读取时，只需要调取 `from_document` 方法加载即可。\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 持久化数据\n",
    "docsearch = Chroma.from_documents(documents, embeddings, persist_directory=\"D:/vector_store\")\n",
    "docsearch.persist()\n",
    "\n",
    "# 加载数据\n",
    "docsearch = Chroma(persist_directory=\"D:/vector_store\", embedding_function=embeddings)\n",
    "\n",
    "```\n",
    "\n",
    "**Pinecone**\n",
    "\n",
    "Pinecone 是一个在线的向量数据库。所以，我可以第一步依旧是注册，然后拿到对应的 api key。https://app.pinecone.io/\n",
    "\n",
    "> 免费版如果索引14天不使用会被自动清除。\n",
    "\n",
    "然后创建我们的数据库：\n",
    "\n",
    "Index Name：这个随意\n",
    "\n",
    "Dimensions：OpenAI 的 text-embedding-ada-002 模型为 OUTPUT DIMENSIONS 为 1536，所以我们这里填 1536\n",
    "\n",
    "Metric：可以默认为 cosine\n",
    "\n",
    "选择starter plan\n",
    "\n",
    "![image-20230405184646314](https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide/raw/main/doc/starter-plan.png)\n",
    "\n",
    "持久化数据和加载数据代码如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YG2MM8jR291W"
   },
   "outputs": [],
   "source": [
    "!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xk5nWwi02y5Y"
   },
   "outputs": [],
   "source": [
    "# 持久化数据\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in split_docs], embeddings, index_name=index_name)\n",
    "\n",
    "# 加载数据\n",
    "docsearch = Pinecone.from_existing_index(index_name, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zl9NP_6r3BTi"
   },
   "source": [
    "一个简单从数据库获取 embeddings，并回答的代码如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSKnlX1AB-__"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "import pinecone\n",
    "\n",
    "# 初始化 pinecone\n",
    "pinecone.init(\n",
    "  api_key=\"你的api key\",\n",
    "  environment=\"你的Environment\"\n",
    ")\n",
    "\n",
    "loader = DirectoryLoader('/content/sample_data/data/', glob='**/*.txt')\n",
    "# 将数据转成 document 对象，每个文件会作为一个 document\n",
    "documents = loader.load()\n",
    "\n",
    "# 初始化加载器\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "# 切割加载的 document\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "index_name=\"liaokong-test\"\n",
    "\n",
    "# 持久化数据\n",
    "# docsearch = Pinecone.from_texts([t.page_content for t in split_docs], embeddings, index_name=index_name)\n",
    "\n",
    "# 加载数据\n",
    "docsearch = Pinecone.from_existing_index(index_name,embeddings)\n",
    "\n",
    "query = \"科大讯飞今年第一季度收入是多少？\"\n",
    "docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=True)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gbwrgOv067X"
   },
   "source": [
    "有几个问题：\n",
    "1. langchain 原生对于中文的支持比较差，有没有快速能够使处理中文的方案\n",
    "2. 存储大量向量上面的方案，性能会有些差，有没有更好的方案\n",
    "3. 对话上下文上，上面的方式并没有很好的解决，有没有好的方案\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okHsTPGy3vxN"
   },
   "source": [
    "# Llamaindex\n",
    "\n",
    "![](https://aimaksen.rarelimiting.com/langchain/l6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gy04AK0W6zeg"
   },
   "source": [
    "![](https://aimaksen.rarelimiting.com/langchain/l7.jpg)\n",
    "\n",
    "![](https://aimaksen.rarelimiting.com/langchain/l8.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cEHisynT0F3E",
    "outputId": "edb8d367-7926-43f0-faf1-34f2df0549ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.8.3-py3-none-any.whl (673 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m673.7/673.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken (from llama-index)\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dataclasses-json (from llama-index)\n",
      "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Collecting langchain>=0.0.262 (from llama-index)\n",
      "  Downloading langchain-0.0.266-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sqlalchemy>=2.0.15 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.0.19)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.23.5)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.2)\n",
      "Collecting openai>=0.26.4 (from llama-index)\n",
      "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
      "Collecting urllib3<2 (from llama-index)\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2023.6.0)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.7.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.11.2)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.7)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama-index) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama-index) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama-index) (4.0.3)\n",
      "Collecting langsmith<0.1.0,>=0.0.21 (from langchain>=0.0.262->llama-index)\n",
      "  Downloading langsmith-0.0.24-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama-index) (2.8.5)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain>=0.0.262->llama-index)\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<2,>=1 (from langchain>=0.0.262->llama-index)\n",
      "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama-index) (2.31.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index)\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai>=0.26.4->llama-index) (4.66.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0.15->llama-index) (2.0.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->llama-index) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2023.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llama-index) (2023.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama-index) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama-index) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama-index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama-index) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama-index) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama-index) (1.3.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index) (23.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.262->llama-index) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.262->llama-index) (2023.7.22)\n",
      "Installing collected packages: urllib3, pydantic, mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, tiktoken, openai, langsmith, dataclasses-json, langchain, llama-index\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.4\n",
      "    Uninstalling urllib3-2.0.4:\n",
      "      Successfully uninstalled urllib3-2.0.4\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.1.1\n",
      "    Uninstalling pydantic-2.1.1:\n",
      "      Successfully uninstalled pydantic-2.1.1\n",
      "Successfully installed dataclasses-json-0.5.14 langchain-0.0.266 langsmith-0.0.24 llama-index-0.8.3 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 pydantic-1.10.12 tiktoken-0.4.0 typing-inspect-0.9.0 urllib3-1.26.16\n",
      "Collecting gradio\n",
      "  Downloading gradio-3.40.1-py3-none-any.whl (20.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.5)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
      "Collecting fastapi (from gradio)\n",
      "  Downloading fastapi-0.101.1-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting gradio-client>=0.4.0 (from gradio)\n",
      "  Downloading gradio_client-0.4.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx (from gradio)\n",
      "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.0.0)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
      "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
      "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.12)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-multipart (from gradio)\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.7.1)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio)\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.0)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.4.0->gradio) (2023.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
      "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.42.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
      "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
      "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
      "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
      "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.6)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio)\n",
      "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.9.2)\n",
      "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.2)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=c66314c3e9364d1282a122ed0253e18187949a3dc9f548493ea7258ba9a173ae\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: pydub, ffmpy, websockets, semantic-version, python-multipart, orjson, markdown-it-py, h11, aiofiles, uvicorn, starlette, mdit-py-plugins, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
      "  Attempting uninstall: markdown-it-py\n",
      "    Found existing installation: markdown-it-py 3.0.0\n",
      "    Uninstalling markdown-it-py-3.0.0:\n",
      "      Successfully uninstalled markdown-it-py-3.0.0\n",
      "  Attempting uninstall: mdit-py-plugins\n",
      "    Found existing installation: mdit-py-plugins 0.4.0\n",
      "    Uninstalling mdit-py-plugins-0.4.0:\n",
      "      Successfully uninstalled mdit-py-plugins-0.4.0\n",
      "Successfully installed aiofiles-23.2.1 fastapi-0.101.1 ffmpy-0.3.1 gradio-3.40.1 gradio-client-0.4.0 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 huggingface-hub-0.16.4 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 orjson-3.9.5 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 uvicorn-0.23.2 websockets-11.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z8GHnpXa8vR1",
    "outputId": "197b92b6-78cc-4a1e-cd91-0e926e99b214"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> punkt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading package punkt to /root/nltk_data...\n",
      "      Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> l\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] bcp47............... BCP-47 Language Tags\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "Hit Enter to continue: \n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "Hit Enter to continue: \n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "Hit Enter to continue: \n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "Hit Enter to continue: \n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet2021......... Open English Wordnet 2021\n",
      "  [ ] wordnet2022......... Open English Wordnet 2022\n",
      "  [ ] wordnet31........... Wordnet 3.1\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "Hit Enter to continue: punkt\n",
      "  [P] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [P] all................. All packages\n",
      "  [P] book................ Everything used in the NLTK Book\n",
      "  [P] popular............. Popular packages\n",
      "  [ ] tests............... Packages for running tests\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages; [P] marks partially installed collections)\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> all-nltk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading collection 'all-nltk'\n",
      "       | \n",
      "       | Downloading package abc to /root/nltk_data...\n",
      "       |   Unzipping corpora/abc.zip.\n",
      "       | Downloading package alpino to /root/nltk_data...\n",
      "       |   Unzipping corpora/alpino.zip.\n",
      "       | Downloading package averaged_perceptron_tagger to\n",
      "       |     /root/nltk_data...\n",
      "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "       | Downloading package averaged_perceptron_tagger_ru to\n",
      "       |     /root/nltk_data...\n",
      "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
      "       | Downloading package basque_grammars to /root/nltk_data...\n",
      "       |   Unzipping grammars/basque_grammars.zip.\n",
      "       | Downloading package bcp47 to /root/nltk_data...\n",
      "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
      "       |   Unzipping corpora/biocreative_ppi.zip.\n",
      "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
      "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "       | Downloading package book_grammars to /root/nltk_data...\n",
      "       |   Unzipping grammars/book_grammars.zip.\n",
      "       | Downloading package brown to /root/nltk_data...\n",
      "       |   Unzipping corpora/brown.zip.\n",
      "       | Downloading package brown_tei to /root/nltk_data...\n",
      "       |   Unzipping corpora/brown_tei.zip.\n",
      "       | Downloading package cess_cat to /root/nltk_data...\n",
      "       |   Unzipping corpora/cess_cat.zip.\n",
      "       | Downloading package cess_esp to /root/nltk_data...\n",
      "       |   Unzipping corpora/cess_esp.zip.\n",
      "       | Downloading package chat80 to /root/nltk_data...\n",
      "       |   Unzipping corpora/chat80.zip.\n",
      "       | Downloading package city_database to /root/nltk_data...\n",
      "       |   Unzipping corpora/city_database.zip.\n",
      "       | Downloading package cmudict to /root/nltk_data...\n",
      "       |   Unzipping corpora/cmudict.zip.\n",
      "       | Downloading package comparative_sentences to\n",
      "       |     /root/nltk_data...\n",
      "       |   Unzipping corpora/comparative_sentences.zip.\n",
      "       | Downloading package comtrans to /root/nltk_data...\n",
      "       | Downloading package conll2000 to /root/nltk_data...\n",
      "       |   Unzipping corpora/conll2000.zip.\n",
      "       | Downloading package conll2002 to /root/nltk_data...\n",
      "       |   Unzipping corpora/conll2002.zip.\n",
      "       | Downloading package conll2007 to /root/nltk_data...\n",
      "       | Downloading package crubadan to /root/nltk_data...\n",
      "       |   Unzipping corpora/crubadan.zip.\n",
      "       | Downloading package dependency_treebank to /root/nltk_data...\n",
      "       |   Unzipping corpora/dependency_treebank.zip.\n",
      "       | Downloading package dolch to /root/nltk_data...\n",
      "       |   Unzipping corpora/dolch.zip.\n",
      "       | Downloading package europarl_raw to /root/nltk_data...\n",
      "       |   Unzipping corpora/europarl_raw.zip.\n",
      "       | Downloading package extended_omw to /root/nltk_data...\n",
      "       | Downloading package floresta to /root/nltk_data...\n",
      "       |   Unzipping corpora/floresta.zip.\n",
      "       | Downloading package framenet_v15 to /root/nltk_data...\n",
      "       |   Unzipping corpora/framenet_v15.zip.\n",
      "       | Downloading package framenet_v17 to /root/nltk_data...\n",
      "       |   Unzipping corpora/framenet_v17.zip.\n",
      "       | Downloading package gazetteers to /root/nltk_data...\n",
      "       |   Unzipping corpora/gazetteers.zip.\n",
      "       | Downloading package genesis to /root/nltk_data...\n",
      "       |   Unzipping corpora/genesis.zip.\n",
      "       | Downloading package gutenberg to /root/nltk_data...\n",
      "       |   Unzipping corpora/gutenberg.zip.\n",
      "       | Downloading package ieer to /root/nltk_data...\n",
      "       |   Unzipping corpora/ieer.zip.\n",
      "       | Downloading package inaugural to /root/nltk_data...\n",
      "       |   Unzipping corpora/inaugural.zip.\n",
      "       | Downloading package indian to /root/nltk_data...\n",
      "       |   Unzipping corpora/indian.zip.\n",
      "       | Downloading package jeita to /root/nltk_data...\n",
      "       | Downloading package kimmo to /root/nltk_data...\n",
      "       |   Unzipping corpora/kimmo.zip.\n",
      "       | Downloading package knbc to /root/nltk_data...\n",
      "       | Downloading package large_grammars to /root/nltk_data...\n",
      "       |   Unzipping grammars/large_grammars.zip.\n",
      "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
      "       |   Unzipping corpora/lin_thesaurus.zip.\n",
      "       | Downloading package mac_morpho to /root/nltk_data...\n",
      "       |   Unzipping corpora/mac_morpho.zip.\n",
      "       | Downloading package machado to /root/nltk_data...\n",
      "       | Downloading package masc_tagged to /root/nltk_data...\n",
      "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
      "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "       | Downloading package maxent_treebank_pos_tagger to\n",
      "       |     /root/nltk_data...\n",
      "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "       | Downloading package moses_sample to /root/nltk_data...\n",
      "       |   Unzipping models/moses_sample.zip.\n",
      "       | Downloading package movie_reviews to /root/nltk_data...\n",
      "       |   Unzipping corpora/movie_reviews.zip.\n",
      "       | Downloading package mte_teip5 to /root/nltk_data...\n",
      "       |   Unzipping corpora/mte_teip5.zip.\n",
      "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
      "       |   Unzipping misc/mwa_ppdb.zip.\n",
      "       | Downloading package names to /root/nltk_data...\n",
      "       |   Unzipping corpora/names.zip.\n",
      "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
      "       | Downloading package nonbreaking_prefixes to\n",
      "       |     /root/nltk_data...\n",
      "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "       | Downloading package nps_chat to /root/nltk_data...\n",
      "       |   Unzipping corpora/nps_chat.zip.\n",
      "       | Downloading package omw to /root/nltk_data...\n",
      "       | Downloading package omw-1.4 to /root/nltk_data...\n",
      "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
      "       |   Unzipping corpora/opinion_lexicon.zip.\n",
      "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
      "       | Downloading package paradigms to /root/nltk_data...\n",
      "       |   Unzipping corpora/paradigms.zip.\n",
      "       | Downloading package pe08 to /root/nltk_data...\n",
      "       |   Unzipping corpora/pe08.zip.\n",
      "       | Downloading package perluniprops to /root/nltk_data...\n",
      "       |   Unzipping misc/perluniprops.zip.\n",
      "       | Downloading package pil to /root/nltk_data...\n",
      "       |   Unzipping corpora/pil.zip.\n",
      "       | Downloading package pl196x to /root/nltk_data...\n",
      "       |   Unzipping corpora/pl196x.zip.\n",
      "       | Downloading package porter_test to /root/nltk_data...\n",
      "       |   Unzipping stemmers/porter_test.zip.\n",
      "       | Downloading package ppattach to /root/nltk_data...\n",
      "       |   Unzipping corpora/ppattach.zip.\n",
      "       | Downloading package problem_reports to /root/nltk_data...\n",
      "       |   Unzipping corpora/problem_reports.zip.\n",
      "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
      "       |   Unzipping corpora/product_reviews_1.zip.\n",
      "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
      "       |   Unzipping corpora/product_reviews_2.zip.\n",
      "       | Downloading package propbank to /root/nltk_data...\n",
      "       | Downloading package pros_cons to /root/nltk_data...\n",
      "       |   Unzipping corpora/pros_cons.zip.\n",
      "       | Downloading package ptb to /root/nltk_data...\n",
      "       |   Unzipping corpora/ptb.zip.\n",
      "       | Downloading package punkt to /root/nltk_data...\n",
      "       |   Package punkt is already up-to-date!\n",
      "       | Downloading package qc to /root/nltk_data...\n",
      "       |   Unzipping corpora/qc.zip.\n",
      "       | Downloading package reuters to /root/nltk_data...\n",
      "       | Downloading package rslp to /root/nltk_data...\n",
      "       |   Unzipping stemmers/rslp.zip.\n",
      "       | Downloading package rte to /root/nltk_data...\n",
      "       |   Unzipping corpora/rte.zip.\n",
      "       | Downloading package sample_grammars to /root/nltk_data...\n",
      "       |   Unzipping grammars/sample_grammars.zip.\n",
      "       | Downloading package semcor to /root/nltk_data...\n",
      "       | Downloading package senseval to /root/nltk_data...\n",
      "       |   Unzipping corpora/senseval.zip.\n",
      "       | Downloading package sentence_polarity to /root/nltk_data...\n",
      "       |   Unzipping corpora/sentence_polarity.zip.\n",
      "       | Downloading package sentiwordnet to /root/nltk_data...\n",
      "       |   Unzipping corpora/sentiwordnet.zip.\n",
      "       | Downloading package shakespeare to /root/nltk_data...\n",
      "       |   Unzipping corpora/shakespeare.zip.\n",
      "       | Downloading package sinica_treebank to /root/nltk_data...\n",
      "       |   Unzipping corpora/sinica_treebank.zip.\n",
      "       | Downloading package smultron to /root/nltk_data...\n",
      "       |   Unzipping corpora/smultron.zip.\n",
      "       | Downloading package snowball_data to /root/nltk_data...\n",
      "       | Downloading package spanish_grammars to /root/nltk_data...\n",
      "       |   Unzipping grammars/spanish_grammars.zip.\n",
      "       | Downloading package state_union to /root/nltk_data...\n",
      "       |   Unzipping corpora/state_union.zip.\n",
      "       | Downloading package stopwords to /root/nltk_data...\n",
      "       |   Unzipping corpora/stopwords.zip.\n",
      "       | Downloading package subjectivity to /root/nltk_data...\n",
      "       |   Unzipping corpora/subjectivity.zip.\n",
      "       | Downloading package swadesh to /root/nltk_data...\n",
      "       |   Unzipping corpora/swadesh.zip.\n",
      "       | Downloading package switchboard to /root/nltk_data...\n",
      "       |   Unzipping corpora/switchboard.zip.\n",
      "       | Downloading package tagsets to /root/nltk_data...\n",
      "       |   Unzipping help/tagsets.zip.\n",
      "       | Downloading package timit to /root/nltk_data...\n",
      "       |   Unzipping corpora/timit.zip.\n",
      "       | Downloading package toolbox to /root/nltk_data...\n",
      "       |   Unzipping corpora/toolbox.zip.\n",
      "       | Downloading package treebank to /root/nltk_data...\n",
      "       |   Unzipping corpora/treebank.zip.\n",
      "       | Downloading package twitter_samples to /root/nltk_data...\n",
      "       |   Unzipping corpora/twitter_samples.zip.\n",
      "       | Downloading package udhr to /root/nltk_data...\n",
      "       |   Unzipping corpora/udhr.zip.\n",
      "       | Downloading package udhr2 to /root/nltk_data...\n",
      "       |   Unzipping corpora/udhr2.zip.\n",
      "       | Downloading package unicode_samples to /root/nltk_data...\n",
      "       |   Unzipping corpora/unicode_samples.zip.\n",
      "       | Downloading package universal_tagset to /root/nltk_data...\n",
      "       |   Unzipping taggers/universal_tagset.zip.\n",
      "       | Downloading package universal_treebanks_v20 to\n",
      "       |     /root/nltk_data...\n",
      "       | Downloading package vader_lexicon to /root/nltk_data...\n",
      "       | Downloading package verbnet to /root/nltk_data...\n",
      "       |   Unzipping corpora/verbnet.zip.\n",
      "       | Downloading package verbnet3 to /root/nltk_data...\n",
      "       |   Unzipping corpora/verbnet3.zip.\n",
      "       | Downloading package webtext to /root/nltk_data...\n",
      "       |   Unzipping corpora/webtext.zip.\n",
      "       | Downloading package wmt15_eval to /root/nltk_data...\n",
      "       |   Unzipping models/wmt15_eval.zip.\n",
      "       | Downloading package word2vec_sample to /root/nltk_data...\n",
      "       |   Unzipping models/word2vec_sample.zip.\n",
      "       | Downloading package wordnet to /root/nltk_data...\n",
      "       | Downloading package wordnet2021 to /root/nltk_data...\n",
      "       | Downloading package wordnet2022 to /root/nltk_data...\n",
      "       |   Unzipping corpora/wordnet2022.zip.\n",
      "       | Downloading package wordnet31 to /root/nltk_data...\n",
      "       | Downloading package wordnet_ic to /root/nltk_data...\n",
      "       |   Unzipping corpora/wordnet_ic.zip.\n",
      "       | Downloading package words to /root/nltk_data...\n",
      "       |   Unzipping corpora/words.zip.\n",
      "       | Downloading package ycoe to /root/nltk_data...\n",
      "       |   Unzipping corpora/ycoe.zip.\n",
      "       | \n",
      "     Done downloading collection all-nltk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#all-nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "aF-0fKs7z07v",
    "outputId": "ae68678d-fd93-4e33-ec7f-c589de19b7fb"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-789f986f8c3e>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                      title=\"专业领域对话机器人\")\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0miface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-789f986f8c3e>\u001b[0m in \u001b[0;36mconstruct_index\u001b[0;34m(directory_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleDirectoryReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPTVectorStoreIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mservice_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/indices/base.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mdocstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_document_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_doc_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             nodes = service_context.node_parser.get_nodes_from_documents(\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/node_parser/simple.py\u001b[0m in \u001b[0;36mget_nodes_from_documents\u001b[0;34m(self, documents, show_progress)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments_with_progress\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 nodes = get_nodes_from_document(\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_text_splitter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/node_parser/node_utils.py\u001b[0m in \u001b[0;36mget_nodes_from_document\u001b[0;34m(document, text_splitter, include_metadata, include_prev_next_rel)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minclude_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_splitter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetadataAwareTextSplitter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             text_splits = text_splitter.split_text_metadata_aware(\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMetadataMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mmetadata_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metadata_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/text_splitter/sentence_splitter.py\u001b[0m in \u001b[0;36msplit_text_metadata_aware\u001b[0;34m(self, text, metadata_str)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mmetadata_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0meffective_chunk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunk_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmetadata_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meffective_chunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msplit_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/text_splitter/sentence_splitter.py\u001b[0m in \u001b[0;36m_split_text\u001b[0;34m(self, text, chunk_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m         ) as event:\n\u001b[1;32m     87\u001b[0m             \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mEventPayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHUNKS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/text_splitter/sentence_splitter.py\u001b[0m in \u001b[0;36m_merge\u001b[0;34m(self, splits, chunk_size)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mcur_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mcur_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcur_len\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Single token exceed chunk size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tiktoken/core.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core_bpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_special\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeEncodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;31m# BPE operates on bytes, but the regex operates on unicode. If we pass a str that is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader, GPTVectorStoreIndex, LLMPredictor, ServiceContext, StorageContext, load_index_from_storage\n",
    "from langchain import OpenAI\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "def construct_index(directory_path):\n",
    "    # 构建索引\n",
    "    num_outputs = 512\n",
    "\n",
    "    llm_predictor = LLMPredictor(llm=OpenAI(client='', temperature=0.7, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "\n",
    "    docs = SimpleDirectoryReader(directory_path).load_data()\n",
    "\n",
    "    index = GPTVectorStoreIndex.from_documents(docs, service_context=service_context)\n",
    "\n",
    "    index.storage_context.persist()\n",
    "\n",
    "    return index\n",
    "\n",
    "def chatbot(input_text):\n",
    "    # 索引对话\n",
    "    storage_context = StorageContext.from_defaults(persist_dir='./storage')\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    query_engine = index.as_query_engine()\n",
    "    response = query_engine.query(input_text)\n",
    "    return response\n",
    "\n",
    "# 对话页面\n",
    "iface = gr.Interface(fn=chatbot,\n",
    "                     inputs=gr.inputs.Textbox(lines=7, label=\"输入你的问题\"),\n",
    "                     outputs=\"text\",\n",
    "                     title=\"专业领域对话机器人\")\n",
    "\n",
    "index = construct_index(\"docs\")\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrTfE6Er7NKJ"
   },
   "source": [
    "![](https://aimaksen.rarelimiting.com/langchain/l9.png)\n",
    "\n",
    "![](https://aimaksen.rarelimiting.com/langchain/l10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Xr9Ic5B7P5m"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhZGQ72NODIZ"
   },
   "source": [
    "#第五个案例：使用GPT3.5模型构建油管频道问答机器人\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itzu-t8A3gAw"
   },
   "source": [
    "在 chatgpt api（也就是 GPT-3.5-Turbo）模型出来后，因钱少活好深受大家喜爱，所以 LangChain 也加入了专属的链和模型，我们来跟着这个例子看下如何使用他。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpAUwIgG4HGy",
    "outputId": "15cc4c4b-bccb-45b1-ae72-978dd1044d3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb\n",
    "!pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "LRan7KZ-MgBv",
    "outputId": "b9a5ae29-f93c-4f90-c9cc-4b59cb7ff793"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8abef6be4180>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYoutubeLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ChatVectorDBChain, ConversationalRetrievalChain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "  ChatPromptTemplate,\n",
    "  SystemMessagePromptTemplate,\n",
    "  HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "# 加载 youtube 频道\n",
    "loader = YoutubeLoader.from_youtube_url('https://www.youtube.com/watch?v=Dj60HHy-Kqk')\n",
    "# 将数据转成 document\n",
    "documents = loader.load()\n",
    "\n",
    "# 初始化文本分割器\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=1000,\n",
    "  chunk_overlap=20\n",
    ")\n",
    "\n",
    "# 分割 youtube documents\n",
    "documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# 初始化 openai embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 将数据存入向量存储\n",
    "vector_store = Chroma.from_documents(documents, embeddings)\n",
    "# 通过向量存储初始化检索器\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "system_template = \"\"\"\n",
    "Use the following context to answer the user's question.\n",
    "If you don't know the answer, say you don't, don't try to make it up. And answer in Chinese.\n",
    "-----------\n",
    "{question}\n",
    "-----------\n",
    "{chat_history}\n",
    "\"\"\"\n",
    "\n",
    "# 构建初始 messages 列表，这里可以理解为是 openai 传入的 messages 参数\n",
    "messages = [\n",
    "  SystemMessagePromptTemplate.from_template(system_template),\n",
    "  HumanMessagePromptTemplate.from_template('{question}')\n",
    "]\n",
    "\n",
    "# 初始化 prompt 对象\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "\n",
    "# 初始化问答链\n",
    "qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0.1,max_tokens=2048),retriever,condense_question_prompt=prompt)\n",
    "\n",
    "\n",
    "chat_history = []\n",
    "while True:\n",
    "  question = input('问题：')\n",
    "  # 开始发送问题 chat_history 为必须参数,用于存储对话历史\n",
    "  result = qa({'question': question, 'chat_history': chat_history})\n",
    "  chat_history.append((question, result['answer']))\n",
    "  print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU4MwTb53pno"
   },
   "source": [
    "我们可以看到他能很准确的围绕这个油管视频进行问答\n",
    "\n",
    "![image-20230406211923672](https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide/raw/main/doc/image-20230406211923672.png)\n",
    "\n",
    "使用流式回答也很方便\n",
    "\n",
    "```python\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(streaming=True, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), verbose=True, temperature=0)\n",
    "resp = chat(chat_prompt_with_values.to_messages())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVApiSmm8ocW"
   },
   "source": [
    "#第六个案例：用 OpenAI 连接万种工具\n",
    "\n",
    "我们主要是结合使用 `zapier` 来实现将万种工具连接起来。\n",
    "\n",
    "所以我们第一步依旧是需要申请账号和他的自然语言 api key。https://zapier.com/l/natural-language-actions\n",
    "\n",
    "他的 api key 虽然需要填写信息申请。但是基本填入信息后，基本可以秒在邮箱里看到审核通过的邮件。\n",
    "\n",
    "然后，我们通过右键里面的连接打开我们的api 配置页面。我们点击右侧的 `Manage Actions` 来配置我们要使用哪些应用。\n",
    "\n",
    "我在这里配置了 Gmail 读取和发邮件的 action，并且所有字段都选的是通过 AI 猜。\n",
    "\n",
    "![image-20230406233319250](https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide/raw/main/doc/image-20230406233319250.png)\n",
    "\n",
    "![image-20230406234827815](https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide/raw/main/doc/image-20230406234827815.png)\n",
    "\n",
    "配置好后，我们开始写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWUYEQM63cXh"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install pytube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMo9CZHr80nW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ZAPIER_NLA_API_KEY\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJp8wx3Q8tDH"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents.agent_toolkits import ZapierToolkit\n",
    "from langchain.utilities.zapier import ZapierNLAWrapper\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=.3)\n",
    "\n",
    "zapier = ZapierNLAWrapper()\n",
    "toolkit = ZapierToolkit.from_zapier_nla_wrapper(zapier)\n",
    "agent = initialize_agent(toolkit.get_tools(), llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "# 我们可以通过打印的方式看到我们都在 Zapier 里面配置了哪些可以用的工具\n",
    "for tool in toolkit.get_tools():\n",
    "  print (tool.name)\n",
    "  print (tool.description)\n",
    "  print (\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-r5lPuV9j_1"
   },
   "outputs": [],
   "source": [
    "agent.run('请用中文总结最后一封\"*********@qq.com\"发给我的邮件。并将总结发送给\"*********@qq.com\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Z_1xtsq4SzN"
   },
   "source": [
    "![image-20230406234712909](doc/image-20230406234712909.png)\n",
    "\n",
    "我们可以看到他成功读取了`******@qq.com`给他发送的最后一封邮件，并将总结的内容又发送给了`******@qq.com`\n",
    "\n",
    "这是我发送给 Gmail 的邮件。\n",
    "\n",
    "![image-20230406234017369](https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide/raw/main/doc/image-20230406234017369.png)\n",
    "\n",
    "这是他发送给 QQ 邮箱的邮件。\n",
    "\n",
    "![image-20230406234800632](https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide/raw/main/doc/image-20230406234800632.png)\n",
    "\n",
    "这只是个小例子，因为 `zapier` 有数以千计的应用，所以我们可以轻松结合 openai api 搭建自己的工作流。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDdWamdTIw4-"
   },
   "source": [
    "# 一些有意思的小Tip\n",
    "\n",
    "一些比较大的知识点都已经讲完了，后面的内容都是一些比较有趣的小例子，当作拓展延伸。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bUGvgAqJfkt"
   },
   "source": [
    "##执行多个chain\n",
    "\n",
    "因为他是链式的，所以他也可以按顺序依次去执行多个 chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQFxSP3rJlHF"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# location 链\n",
    "llm = OpenAI(temperature=1)\n",
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# meal 链\n",
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# 通过 SimpleSequentialChain 串联起来，第一个答案会被替换第二个中的user_meal，然后再进行询问\n",
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)\n",
    "review = overall_chain.run(\"Rome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lkh9Z58h4i1i"
   },
   "source": [
    "![image-20230406000133339](https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide/raw/main/doc/image-20230406000133339.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QVu6_HVJTlW"
   },
   "source": [
    "##结构化输出\n",
    "\n",
    "有时候我们希望输出的内容不是文本，而是像 json 那样结构化的数据。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwEocODtt5Tm"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# 告诉他我们生成的内容需要哪些字段，每个字段类型式啥\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
    "]\n",
    "\n",
    "# 初始化解析器\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# 生成的格式提示符\n",
    "# {\n",
    "#\t\"bad_string\": string  // This a poorly formatted user input string\n",
    "#\t\"good_string\": string  // This is your response, a reformatted response\n",
    "#}\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "# 将我们的格式描述嵌入到 prompt 中去，告诉 llm 我们需要他输出什么样格式的内容\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
    "llm_output = llm(promptValue)\n",
    "\n",
    "# 使用解析器进行解析生成的内容\n",
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00kLaQZWS-Jq"
   },
   "source": [
    "##爬取网页并输出JSON数据\n",
    "\n",
    "有些时候我们需要爬取一些<mark style=\"color:red;\">**结构性比较强**</mark>的网页，并且需要将网页中的信息以JSON的方式返回回来。\n",
    "\n",
    "我们就可以使用 `LLMRequestsChain` 类去实现，具体可以参考下面代码\n",
    "\n",
    "> 为了方便理解，我在例子中直接使用了Prompt的方法去格式化输出结果，而没用使用上个案例中用到的 `StructuredOutputParser`去格式化，也算是提供了另外一种格式化的思路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAkS_wX8IaA1"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMRequestsChain, LLMChain\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "template = \"\"\"在 >>> 和 <<< 之间是网页的返回的HTML内容。\n",
    "网页是新浪财经A股上市公司的公司简介。\n",
    "请抽取参数请求的信息。\n",
    "\n",
    ">>> {requests_result} <<<\n",
    "请使用如下的JSON格式返回数据\n",
    "{{\n",
    "  \"company_name\":\"a\",\n",
    "  \"company_english_name\":\"b\",\n",
    "  \"issue_price\":\"c\",\n",
    "  \"date_of_establishment\":\"d\",\n",
    "  \"registered_capital\":\"e\",\n",
    "  \"office_address\":\"f\",\n",
    "  \"Company_profile\":\"g\"\n",
    "\n",
    "}}\n",
    "Extracted:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"requests_result\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "chain = LLMRequestsChain(llm_chain=LLMChain(llm=llm, prompt=prompt))\n",
    "inputs = {\n",
    "  \"url\": \"https://vip.stock.finance.sina.com.cn/corp/go.php/vCI_CorpInfo/stockid/600519.phtml\"\n",
    "}\n",
    "\n",
    "response = chain(inputs)\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNVMllwE44Ij"
   },
   "source": [
    "我们可以看到，他很好的将格式化后的结果输出了出来\n",
    "\n",
    "<figure><img src=\"https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide/raw/main/doc/image-20230510234934.png\" alt=\"\"><figcaption></figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dh9Zxt6NqAY"
   },
   "source": [
    "##自定义工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHMHz8vfNs6w"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMMathChain, SerpAPIWrapper\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# 初始化搜索链和计算链\n",
    "search = SerpAPIWrapper()\n",
    "llm_math_chain = LLMMathChain(llm=llm, verbose=True)\n",
    "\n",
    "# 生成一个功能列表，指明这个 agent 里面都有哪些可用工具\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for when you need to answer questions about math\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# 初始化 agent\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# 执行 agent\n",
    "agent.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAUOKdO_5ElZ"
   },
   "source": [
    "![image-20230406002117283](https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide/raw/main/doc/image-20230406002117283.png)\n",
    "\n",
    "自定义工具里面有个比较有意思的地方，使用哪个工具的权重是靠 `工具中描述内容` 来实现的，和我们之前编程靠数值来控制权重完全不同。\n",
    "\n",
    "比如 Calculator 在描述里面写到，如果你问关于数学的问题就用他这个工具。我们就可以在上面的执行过程中看到，他在我们请求的 prompt 中数学的部分，就选用了Calculator 这个工具进行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOIICEUZvjR0"
   },
   "source": [
    "##使用Memory实现一个带记忆的对话机器人\n",
    "\n",
    "上一个例子我们使用的是通过自定义一个列表来存储对话的方式来保存历史的。\n",
    "\n",
    "当然，你也可以使用自带的 memory 对象来实现这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9qG5EGbpviKl",
    "outputId": "1d8766bb-bf1b-48bb-cd3a-3984a3a94384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='中国的首都是北京。' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# 初始化 MessageHistory 对象\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "# 给 MessageHistory 对象添加对话内容\n",
    "history.add_ai_message(\"你好！\")\n",
    "history.add_user_message(\"中国的首都是哪里？\")\n",
    "\n",
    "# 执行对话\n",
    "ai_response = chat(history.messages)\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0E36YZf5WhE"
   },
   "source": [
    "### **使用 Hugging Face 模型**\n",
    "\n",
    "使用 Hugging Face 模型之前，需要先设置环境变量\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = ''\n",
    "```\n",
    "\n",
    "使用在线的 Hugging Face 模型\n",
    "\n",
    "```python\n",
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm = HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0, \"max_length\":64})\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "print(llm_chain.run(question))\n",
    "```\n",
    "\n",
    "将 Hugging Face 模型直接拉到本地使用\n",
    "\n",
    "```python\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = 'google/flan-t5-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(local_llm('What is the capital of France? '))\n",
    "\n",
    "\n",
    "template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=local_llm)\n",
    "question = \"What is the capital of England?\"\n",
    "print(llm_chain.run(question))\n",
    "```\n",
    "\n",
    "将模型拉到本地使用的好处：\n",
    "\n",
    "* 训练模型\n",
    "* 可以使用本地的 GPU\n",
    "* 有些模型无法在 Hugging Face 运行\n",
    "\n",
    "### **通过自然语言执行SQL命令**\n",
    "\n",
    "我们通过 `SQLDatabaseToolkit` 或者 `SQLDatabaseChain` 都可以实现执行SQL命令的操作\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///../notebooks/Chinook.db\")\n",
    "toolkit = SQLDatabaseToolkit(db=db)\n",
    "\n",
    "agent_executor = create_sql_agent(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    toolkit=toolkit,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor.run(\"Describe the playlisttrack table\")\n",
    "```\n",
    "\n",
    "```python\n",
    "from langchain import OpenAI, SQLDatabase, SQLDatabaseChain\n",
    "\n",
    "db = SQLDatabase.from_uri(\"mysql+pymysql://root:root@127.0.0.1/chinook\")\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)\n",
    "db_chain.run(\"How many employees are there?\")\n",
    "```\n",
    "\n",
    "这里可以参考这两篇文档：\n",
    "\n",
    "[https://python.langchain.com/en/latest/modules/agents/toolkits/examples/sql\\_database.html](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/sql\\_database.html)\n",
    "\n",
    "[https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html](https://python.langchain.com/en/latest/modules/chains/examples/sqlite.html)\n",
    "\n",
    "## 总结\n",
    "\n",
    "所有的案例都基本已经结束了，希望大家能通过这篇文章的学习有所收获。这篇文章只是对 LangChain 一个初级的讲解，高级的功能希望大家继续探索。\n",
    "\n",
    "并且因为 LangChain 迭代极快，所以后面肯定会随着AI继续的发展，还会迭代出更好用的功能，所以我非常看好这个开源库。\n",
    "\n",
    "希望大家能结合 LangChain 开发出更有创意的产品，而不仅仅只搞一堆各种一键搭建chatgpt聊天客户端的那种产品。\n",
    "\n",
    "这篇标题后面加了个 `01` 是我希望这篇文章只是一个开始，后面如何出现了更好的技术我还是希望能继续更新下去这个系列。\n",
    "\n",
    "本文章的所有范例代码都在这里，祝大家学习愉快。\n",
    "\n",
    "https://colab.research.google.com/drive/1ArRVMiS-YkhUlobHrU6BeS8fF57UeaPQ?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrMnnHig5XxW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
