{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICcKuSQWd9uh"
      },
      "source": [
        "# 查询引擎\n",
        "\n",
        "查询引擎建立在我们之前讨论过的检索器和响应合成器之上\n",
        "\n",
        "查询引擎允许您对数据提出问题。LlamaIndex支持多种查询引擎。我们现在将讨论其中一些\n",
        "\n",
        "1. 路由查询引擎\n",
        "2. 检索器路由查询引擎\n",
        "3. 联合问答摘要查询引擎\n",
        "4. 子问题查询引擎\n",
        "5. 具有混合搜索的自定义检索器"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvzm39FGgki8"
      },
      "source": [
        "### 路由查询引擎\n",
        "\n",
        "路由查询引擎帮助您在多个索引之上创建一个查询引擎。根据问题与索引描述的相似性选择索引\n",
        "\n",
        "例如，如果您有一个了解数学的索引和另一个了解物理的索引，您可以通过创建路由查询引擎在这两个索引上创建一个组合查询引擎\n",
        "\n",
        "让我们通过一个示例来理解"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWpuvOgPd5Fs"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a4v2RkfKhO5b"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m169xw8WhYpg"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().handlers = []\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    ListIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    StorageContext,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9s80nf9h8TR"
      },
      "source": [
        "### 下载相关数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1T7MPyvhfXc",
        "outputId": "e20bc493-66da-4425-afe3-bf42c8733923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-07-22 16:56:24--  https://github.com/jerryjliu/llama_index/blob/main/examples/paul_graham_essay/data/paul_graham_essay.txt\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84944 (83K) [text/plain]\n",
            "Saving to: ‘paul_graham_essay.txt’\n",
            "\n",
            "\rpaul_graham_essay.t   0%[                    ]       0  --.-KB/s               \rpaul_graham_essay.t 100%[===================>]  82.95K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-07-22 16:56:25 (5.35 MB/s) - ‘paul_graham_essay.txt’ saved [84944/84944]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!wget https://github.com/jerryjliu/llama_index/blob/main/examples/paul_graham_essay/data/paul_graham_essay.txt\n",
        "!mv paul_graham_essay.txt data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ntoicl2Yh6bh"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"./data/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i9r25o7XiBgo"
      },
      "outputs": [],
      "source": [
        "service_context = ServiceContext.from_defaults(chunk_size=1024)\n",
        "nodes = service_context.node_parser.get_nodes_from_documents(documents)\n",
        "storage_context = StorageContext.from_defaults()\n",
        "storage_context.docstore.add_documents(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Mxx9XzlLicir"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = \"your-openai-key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sc47wyWYiC1Y"
      },
      "outputs": [],
      "source": [
        "list_index = ListIndex(nodes, storage_context=storage_context)\n",
        "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJQvNyqxpmjY"
      },
      "source": [
        "我们正在创建两个查询引擎，一个用于摘要任务，另一个用于问答\n",
        "\n",
        "对于摘要，我们使用具有响应模式tree_summarize的列表索引，它可以从输入数据构建树状数据结构，并自底向上创建数据摘要\n",
        "\n",
        "对于问答，我们将使用VectorIndex，它可以获取用于创建响应的前几个相关文档"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "u1v0jsn0iSV0"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools.query_engine import QueryEngineTool\n",
        "list_query_engine = list_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()\n",
        "list_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=list_query_engine,\n",
        "    description=\"Useful for summarization questions related to Paul Graham eassy on What I Worked On.\",\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=\"Useful for retrieving specific context from Paul Graham essay on What I Worked On.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVdXJfMcqB0w"
      },
      "source": [
        "现在我们将在这两个查询引擎之上构建一个RouterQueryEngine，它可以根据输入查询选择其中一个引擎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HB_n-65oiyJl"
      },
      "outputs": [],
      "source": [
        "from llama_index.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.selectors.llm_selectors import LLMSingleSelector, LLMMultiSelector\n",
        "from llama_index.selectors.pydantic_selectors import (\n",
        "    PydanticMultiSelector,\n",
        "    PydanticSingleSelector,\n",
        ")\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=PydanticSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        list_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-xAWjHOqPto"
      },
      "source": [
        "由于问题是一个摘要问题，它选择了列表索引并创建了一个摘要来提供响应"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WcCcJ7Ri0-f",
        "outputId": "5f25eb57-1221-4140-d49d-e5ac573e6612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The document is a collection of essays written by Paul Graham, reflecting on his journey from working on Viaweb to starting Y Combinator, painting, writing essays, working on Lisp, and writing Bel. He discusses topics such as the difficulty of carrying heavy items, the problems with running a forum and writing essays, leaving Y Combinator, and the concept of invented versus discovered. He also thanks several people for reading drafts of the essays.\n"
          ]
        }
      ],
      "source": [
        "print(query_engine.query(\"What is the summary of the document?\").response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxnX1KLJqZv9"
      },
      "source": [
        "由于这里是一个问答问题，选择了一个向量索引来给出适当的响应。因此，根据输入查询选择了正确的查询引擎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiRSLr_8pW6N",
        "outputId": "2a7f1413-25d5-4833-b26b-d6e1c0869b92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Paul Graham decided to paint. He wanted to see how good he could get if he dedicated himself to painting and left his job at Y Combinator. He recruited Dan Giffin and two undergrads to help him build a web app for making web apps, and he moved to Cambridge to start the company.\n"
          ]
        }
      ],
      "source": [
        "print(query_engine.query(\"What did Paul Graham do after RICS?\").response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqdR-jevqraG"
      },
      "source": [
        "### 检索器路由查询引擎\n",
        "\n",
        "检索器路由查询引擎在功能上类似于上述内容。唯一的区别在于路由器由检索器驱动\n",
        "\n",
        "使用向量索引驱动的检索器的优势在于，可以作为路由器的一部分保留的查询引擎数量不再受模型上下文长度的限制。因此，可以使用任意数量的查询引擎作为检索器的一部分"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "P_uz6d3Ppfuy"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gAKwsvwarXaT"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    ListIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    StorageContext,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GHdjIi7AraPj"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"./data/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RJQGc79MreWr"
      },
      "outputs": [],
      "source": [
        "service_context = ServiceContext.from_defaults(chunk_size=1024)\n",
        "nodes = service_context.node_parser.get_nodes_from_documents(documents)\n",
        "storage_context = StorageContext.from_defaults()\n",
        "storage_context.docstore.add_documents(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FVMo6LqOrh6g"
      },
      "outputs": [],
      "source": [
        "list_index = ListIndex(nodes, storage_context=storage_context)\n",
        "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OYAiZ47trkE2"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools.query_engine import QueryEngineTool\n",
        "\n",
        "list_query_engine = list_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\", use_async=True\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\", use_async=True\n",
        ")\n",
        "\n",
        "list_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=list_query_engine,\n",
        "    description=\"Useful for questions asking for a biography of the author.\",\n",
        ")\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=\"Useful for retrieving specific snippets from the author's life, like his time in college, his time in YC, or more.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVhMVcXusvET"
      },
      "source": [
        "到目前为止，流程与路由查询引擎相同，我们创建了两个查询引擎。现在的区别在于，我们使用在此处提到的ObjectIndex构建了一个使用向量索引的路由查询引擎。\n",
        "\n",
        "ObjectIndex是一种基础索引数据结构，可以将QueryEngineTool对象序列化为索引。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lHJ50b4PrnDZ"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "from llama_index.objects import ObjectIndex, SimpleToolNodeMapping\n",
        "\n",
        "tool_mapping = SimpleToolNodeMapping.from_objects([list_tool, vector_tool])\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    [list_tool, vector_tool],\n",
        "    tool_mapping,\n",
        "    VectorStoreIndex,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Hl7dII5rrrFp"
      },
      "outputs": [],
      "source": [
        "from llama_index.query_engine import ToolRetrieverRouterQueryEngine\n",
        "\n",
        "query_engine = ToolRetrieverRouterQueryEngine(obj_index.as_retriever())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EG8axa9ArtES"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"What is a biography of the author's life?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8OLDIZzruvA",
        "outputId": "7cb6eba3-aaa0-4d9f-ca9b-f692c481b76d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Paul Graham is a computer scientist, programmer, and entrepreneur who was born in England. He moved to the United States in the 1980s to pursue a PhD in computer science at Harvard University, where he wrote the book On Lisp and worked on a project called Bel. He then moved to Italy to study art at the Accademia di Belli Arti in Florence. After returning to the United States, he wrote essays and eventually moved back to England with his family. In 2019, he finished Bel and wrote a series of essays. In 2020, he wrote an essay about how he chooses what to work on.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UZgQGkrLr1-v"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"What did Paul Graham do during his time in college?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91M576qmr7mW",
        "outputId": "8cc6e0b3-a2be-4ee9-fbd9-9d072feb1734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Paul Graham studied computer science and took art classes at Harvard University. He applied to two art schools, RISD and the Accademia di Belli Arti in Florence, and was accepted to RISD. He wrote a dissertation on applications of continuations in order to graduate from Harvard. He then attended RISD, where he took the foundation classes in drawing, color, and design. He also took the entrance exam for the Accademia di Belli Arti in Florence and passed.\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbY29gsTt0RJ"
      },
      "source": [
        "### 子问题查询引擎\n",
        "\n",
        "子问题查询引擎解决了回答复杂查询的问题，它将复杂查询分解为每个相关数据源的子问题，然后收集所有中间响应并合成最终响应。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NduxyFdXr-hN"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.query_engine import SubQuestionQueryEngine\n",
        "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
        "from llama_index import ServiceContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "aKt5Y5o7uWwU"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "pg_essay = SimpleDirectoryReader(input_dir=\"./data/\").load_data()\n",
        "\n",
        "# build index and query engine\n",
        "query_engine = VectorStoreIndex.from_documents(pg_essay).as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zwacdDsnuaqg"
      },
      "outputs": [],
      "source": [
        "query_engine_tools = [\n",
        "    QueryEngineTool(\n",
        "        query_engine=query_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=\"pg_essay\", description=\"Paul Graham essay on What I Worked On\"\n",
        "        ),\n",
        "    )\n",
        "]\n",
        "\n",
        "# Using the LlamaDebugHandler to print the trace of the sub questions\n",
        "# captured by the SUB_QUESTION callback event type\n",
        "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
        "callback_manager = CallbackManager([llama_debug])\n",
        "service_context = ServiceContext.from_defaults(callback_manager=callback_manager)\n",
        "query_engine = SubQuestionQueryEngine.from_defaults(\n",
        "    query_engine_tools=query_engine_tools,\n",
        "    service_context=service_context,\n",
        "    use_async=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yIDxeppuesE",
        "outputId": "b698ca00-2f40-4f17-e635-84a5b99db81b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 2 sub questions.\n",
            "\u001b[36;1m\u001b[1;3m[pg_essay] Q: What did Paul Graham work on before YC?\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[pg_essay] Q: What did Paul Graham work on after YC?\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[pg_essay] A: \n",
            "Paul Graham continued to work on writing essays and working on YC. He also worked on Hacker News, which was originally meant to be a news aggregator for startup founders and was called Startup News. He wrote all of YC's internal software in Arc, but gradually stopped working on Arc due to lack of time and the infrastructure depending on it.\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[pg_essay] A: \n",
            "Before YC, Paul Graham worked on hacking, writing essays, and Arc, a programming language. He also ran a weekly dinner at his building in Cambridge and created the Summer Founders Program, which invited undergrads to apply for startup funding.\n",
            "\u001b[0m**********\n",
            "Trace: query\n",
            "    |_llm ->  3.388304 seconds\n",
            "    |_sub_questions ->  3.653011 seconds\n",
            "    |_synthesize ->  3.346864 seconds\n",
            "      |_llm ->  3.342244 seconds\n",
            "**********\n"
          ]
        }
      ],
      "source": [
        "response = await query_engine.aquery(\n",
        "    \"How was Paul Grahams life different before and after YC?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCkuvlChuhFj",
        "outputId": "4fe8f02f-f662-46ce-880b-99bead97a29b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Paul Graham's life changed significantly after YC. Before YC, he was mainly focused on hacking, writing essays, and developing Arc, a programming language. He also ran a weekly dinner at his building in Cambridge and created the Summer Founders Program. After YC, he continued to write essays and work on YC, but he also worked on Hacker News and wrote all of YC's internal software in Arc. He gradually stopped working on Arc due to lack of time and the infrastructure depending on it.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AVI6Po_u2XI"
      },
      "source": [
        "### 联合问答摘要查询引擎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "SmVp-TytusrT"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "q8OaRq46u4zW"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3zen7XaFu6Y0"
      },
      "outputs": [],
      "source": [
        "from llama_index.composability.joint_qa_summary import QASummaryQueryEngineBuilder\n",
        "from llama_index import SimpleDirectoryReader, ServiceContext, LLMPredictor\n",
        "from llama_index.response.notebook_utils import display_response\n",
        "from llama_index.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CMGBt9u5u8PL"
      },
      "outputs": [],
      "source": [
        "reader = SimpleDirectoryReader(\"./data/\")\n",
        "documents = reader.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "U6bzq6EEvAHk"
      },
      "outputs": [],
      "source": [
        "service_context = ServiceContext.from_defaults(chunk_size=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "BNVuJNZcvJqe"
      },
      "outputs": [],
      "source": [
        "query_engine_builder = QASummaryQueryEngineBuilder(service_context=service_context)\n",
        "query_engine = query_engine_builder.build_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zciG7c6XvPHx"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\n",
        "    \"Can you give me a summary of the author's life?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeGPyRb_vTcB",
        "outputId": "236fa563-f786-412d-f80a-9199426be795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The author, Paul Graham, is a computer scientist and artist. He was born in England and moved to the US to pursue a PhD in computer science. While in grad school, he worked on On Lisp and wrote a dissertation on applications of continuations. He then applied to art schools and was accepted to RISD and the Accademia di Belli Arti in Florence. He moved to Florence and passed the entrance exam, and then spent 3 months writing essays. He then worked on Bel, an interpreter written in itself, for years, while living in England. In 2019, Bel was finished and he wrote essays about topics he had stacked up. He now lives in England and is thinking about what to work on next.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Rsa1FrwdvVmB"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\n",
        "    \"What did the author do during his time in art school?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gef9lLFvalP",
        "outputId": "26f7517d-80c9-47e2-f72b-0958500e573d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The author took art classes at Harvard, applied to two art schools (RISD and the Accademia di Belli Arti in Florence), took the entrance exam for the Accademia di Belli Arti in Florence, attended the RISD foundation program, and painted still lives in his bedroom at night while attending the Accademia di Belli Arti in Florence. He also learned Italian and studied under professor Ulivi.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkjOnu85vl0T"
      },
      "source": [
        "### 具有混合搜索的自定义检索器\n",
        "\n",
        "基于关键词的搜索是信息检索系统中使用的初始搜索形式。最近，我们有基于向量数据库的搜索，它基于语义相似性工作。\n",
        "\n",
        "并不总是必要的，基于向量数据库的搜索在特定查询上表现比基于关键词的搜索更好。情况也可能相反。\n",
        "\n",
        "因此，为了克服这一点，我们可以使用混合搜索，从而获得两全其美的结果。让我们讨论如何在LlamaIndex中使用自定义检索器实现这一点"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ytqxBpDl1bS9"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleKeywordTableIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    StorageContext,\n",
        ")\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "NvLaGasX1fgs"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"./data/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "tr3ju5Tr1i2w"
      },
      "outputs": [],
      "source": [
        "service_context = ServiceContext.from_defaults(chunk_size=1024)\n",
        "node_parser = service_context.node_parser\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "storage_context = StorageContext.from_defaults()\n",
        "storage_context.docstore.add_documents(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWaHLCnj1npS",
        "outputId": "c4622cae-2fc3-4d7c-a1c6-6d0de52f55c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "keyword_index = SimpleKeywordTableIndex(nodes, storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "I4MyW7MN1q87"
      },
      "outputs": [],
      "source": [
        "from llama_index import QueryBundle\n",
        "\n",
        "# import NodeWithScore\n",
        "from llama_index.schema import NodeWithScore\n",
        "\n",
        "# Retrievers\n",
        "from llama_index.retrievers import (\n",
        "    BaseRetriever,\n",
        "    VectorIndexRetriever,\n",
        "    KeywordTableSimpleRetriever,\n",
        ")\n",
        "\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "SMXnlszC1uBY"
      },
      "outputs": [],
      "source": [
        "class CustomRetriever(BaseRetriever):\n",
        "    \"\"\"Custom retriever that performs both semantic search and hybrid search.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vector_retriever: VectorIndexRetriever,\n",
        "        keyword_retriever: KeywordTableSimpleRetriever,\n",
        "        mode: str = \"AND\",\n",
        "    ) -> None:\n",
        "        \"\"\"Init params.\"\"\"\n",
        "\n",
        "        self._vector_retriever = vector_retriever\n",
        "        self._keyword_retriever = keyword_retriever\n",
        "        if mode not in (\"AND\", \"OR\"):\n",
        "            raise ValueError(\"Invalid mode.\")\n",
        "        self._mode = mode\n",
        "\n",
        "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
        "        \"\"\"Retrieve nodes given query.\"\"\"\n",
        "\n",
        "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
        "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
        "\n",
        "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
        "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
        "\n",
        "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
        "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
        "\n",
        "        if self._mode == \"AND\":\n",
        "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
        "        else:\n",
        "            retrieve_ids = vector_ids.union(keyword_ids)\n",
        "\n",
        "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
        "        return retrieve_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "M7pvpGCS1yQJ"
      },
      "outputs": [],
      "source": [
        "from llama_index import get_response_synthesizer\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "\n",
        "# define custom retriever\n",
        "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=2)\n",
        "keyword_retriever = KeywordTableSimpleRetriever(index=keyword_index)\n",
        "custom_retriever = CustomRetriever(vector_retriever, keyword_retriever)\n",
        "\n",
        "# define response synthesizer\n",
        "response_synthesizer = get_response_synthesizer()\n",
        "\n",
        "# assemble query engine\n",
        "custom_query_engine = RetrieverQueryEngine(\n",
        "    retriever=custom_retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        ")\n",
        "\n",
        "# vector query engine\n",
        "vector_query_engine = RetrieverQueryEngine(\n",
        "    retriever=vector_retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        ")\n",
        "# keyword query engine\n",
        "keyword_query_engine = RetrieverQueryEngine(\n",
        "    retriever=keyword_retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "VRuQY4wl10_Q"
      },
      "outputs": [],
      "source": [
        "response = custom_query_engine.query(\"What did the author do during his time at YC?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSStcZ_c127F",
        "outputId": "84511db8-4acf-4921-b27c-f5efb271e7ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The author worked on YC, writing essays, developing internal software in Arc, and creating Hacker News. He also helped select and support founders, resolve disputes between cofounders, and fight with people who maltreated the startups. He worked hard, even at the parts he didn't like, and eventually handed YC over to someone else. After his mother's death, he checked out of YC and decided to pursue painting.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNgytkgE1PfjiuoA/fJLGnA",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
