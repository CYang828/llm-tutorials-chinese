{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain åº”ç”¨å¼€å‘æŒ‡å—ï¼ˆåŸºç¡€ï¼‰  ğŸ‘¨â€ğŸ³ğŸ‘©â€ğŸ³"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*æœ¬æ•™ç¨‹åŸºäº[LangChainæ¦‚å¿µæ–‡æ¡£](https://docs.langchain.com/docs/)*\n",
    "\n",
    "**ç›®æ ‡ï¼š** é€šè¿‡[ELI5](https://www.dictionary.com/e/slang/eli5/#:~:text=ELI5%20is%20short%20for%20%E2%80%9CExplain,a%20complicated%20question%20or%20problem.)ç¤ºä¾‹å’Œä»£ç ç‰‡æ®µï¼Œæä¾›å¯¹LangChainç»„ä»¶å’Œç”¨ä¾‹çš„åˆæ­¥ç†è§£ã€‚æœ‰å…³ç”¨ä¾‹ï¼Œè¯·æŸ¥çœ‹[ç¬¬2éƒ¨åˆ†](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%202%20-%20Use%20Cases.ipynb)ã€‚æŸ¥çœ‹æœ¬ç¬”è®°æœ¬çš„[è§†é¢‘æ•™ç¨‹](https://www.youtube.com/watch?v=2xxziIWmaSA)ã€‚\n",
    "\n",
    "**é“¾æ¥ï¼š**\n",
    "* [LCæ¦‚å¿µæ–‡æ¡£](https://docs.langchain.com/docs/)\n",
    "* [LC Pythonæ–‡æ¡£](https://python.langchain.com/en/latest/)\n",
    "* [LC Javascript/Typescriptæ–‡æ¡£](https://js.langchain.com/docs/)\n",
    "* [LC Discord](https://discord.gg/6adMQxSpJS)\n",
    "* [www.langchain.com](https://langchain.com/)\n",
    "* [LC Twitter](https://twitter.com/LangChainAI)\n",
    "\n",
    "### **ä»€ä¹ˆæ˜¯LangChainï¼Ÿ**\n",
    "> LangChainæ˜¯ä¸€ä¸ªåŸºäºè¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºå¼€å‘æ¡†æ¶ã€‚\n",
    "\n",
    "**~~TL~~DR**ï¼šLangChainä½¿ä¸AIæ¨¡å‹çš„å·¥ä½œå’Œæ„å»ºä¸­çš„å¤æ‚éƒ¨åˆ†å˜å¾—æ›´å®¹æ˜“ã€‚å®ƒé€šè¿‡ä¸¤ç§æ–¹å¼å®ç°ï¼š\n",
    "\n",
    "1. **é›†æˆ** - å°†å¤–éƒ¨æ•°æ®ï¼ˆä¾‹å¦‚æ‚¨çš„æ–‡ä»¶ã€å…¶ä»–åº”ç”¨ç¨‹åºå’ŒAPIæ•°æ®ï¼‰å¼•å…¥åˆ°æ‚¨çš„LLMsä¸­\n",
    "2. **ä»£ç†** - å…è®¸æ‚¨çš„LLMsé€šè¿‡å†³ç­–ä¸å…¶ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚ä½¿ç”¨LLMså¸®åŠ©å†³å®šä¸‹ä¸€æ­¥é‡‡å–çš„è¡ŒåŠ¨\n",
    "\n",
    "### **ä¸ºä»€ä¹ˆé€‰æ‹©LangChainï¼Ÿ**\n",
    "1. **ç»„ä»¶** - LangChainä½¿å¾—æ˜“äºæ›¿æ¢ä¸è¯­è¨€æ¨¡å‹å·¥ä½œæ‰€å¿…éœ€çš„æŠ½è±¡å’Œç»„ä»¶ã€‚\n",
    "\n",
    "2. **å®šåˆ¶é“¾** - LangChainæä¾›äº†å¯¹ä½¿ç”¨å’Œå®šåˆ¶â€œé“¾â€ï¼ˆä¸€ç³»åˆ—ä¸²è”çš„æ“ä½œï¼‰çš„å¼€ç®±å³ç”¨æ”¯æŒã€‚\n",
    "\n",
    "3. **é€Ÿåº¦ ğŸš¢** - è¿™ä¸ªå›¢é˜Ÿäº¤ä»˜é€Ÿåº¦æå¿«ã€‚æ‚¨å°†å§‹ç»ˆäº†è§£æœ€æ–°çš„LLMåŠŸèƒ½ã€‚\n",
    "\n",
    "4. **ç¤¾åŒº ğŸ‘¥** - å‡ºè‰²çš„Discordå’Œç¤¾åŒºæ”¯æŒã€è§é¢ä¼šã€é»‘å®¢é©¬æ‹‰æ¾ç­‰ã€‚\n",
    "\n",
    "å°½ç®¡LLMså¯èƒ½å¾ˆç®€å•ï¼ˆè¾“å…¥æ–‡æœ¬ï¼Œè¾“å‡ºæ–‡æœ¬ï¼‰ï¼Œä½†ä¸€æ—¦æ‚¨å¼€å‘æ›´å¤æ‚çš„åº”ç”¨ç¨‹åºï¼Œæ‚¨å¾ˆå¿«å°±ä¼šé‡åˆ°LangChainå¸®åŠ©è§£å†³çš„æ‘©æ“¦ç‚¹ã€‚\n",
    "\n",
    "*æ³¨æ„ï¼šæœ¬æ•™ç¨‹ä¸ä¼šæ¶µç›–LangChainçš„æ‰€æœ‰æ–¹é¢ã€‚å…¶å†…å®¹ç»è¿‡ç²¾å¿ƒç­–åˆ’ï¼Œæ—¨åœ¨è®©æ‚¨å°½å¿«å¼€å§‹æ„å»ºå’Œäº§ç”Ÿå½±å“ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[LangChainæ¦‚å¿µæ–‡æ¡£](https://docs.langchain.com/docs/)*\n",
    "\n",
    "*2023å¹´10æœˆæ›´æ–°ï¼šæ­¤ç¬”è®°æœ¬å·²ä»å…¶åŸå§‹å½¢å¼è¿›è¡Œäº†æ‰©å±•*\n",
    "\n",
    "æ‚¨éœ€è¦ä¸€ä¸ªOpenAI APIå¯†é’¥æ‰èƒ½è·Ÿéšæœ¬æ•™ç¨‹ã€‚æ‚¨å¯ä»¥å°†å…¶ä½œä¸ºç¯å¢ƒå˜é‡ï¼Œæ”¾åœ¨æ­¤Jupyterç¬”è®°æœ¬æ‰€åœ¨çš„.envæ–‡ä»¶ä¸­ï¼Œæˆ–è€…å°†å…¶æ’å…¥åˆ°ä¸‹é¢çš„'YourAPIKey'å¤„ã€‚å¦‚æœå¯¹æ­¤æœ‰ç–‘é—®ï¼Œè¯·å°†è¿™äº›è¯´æ˜æ”¾å…¥[ChatGPT](https://chat.openai.com/)ã€‚\n",
    "\n",
    "[å¦‚ä½•è·å– OpenAI APIKeyï¼Ÿ](./è·å–OpenAI%20APIKey.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key=os.getenv('OPENAI_API_KEY', 'YourAPIKey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain ç»„ä»¶\n",
    "\n",
    "## æ¨¡å¼ - å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŸºæœ¬å·¥ä½œåŸç†\n",
    "\n",
    "### **æ–‡æœ¬**\n",
    "ä¸LLMsè¿›è¡Œè‡ªç„¶è¯­è¨€äº¤äº’çš„æ–¹å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What day comes after Friday?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You'll be working with simple strings (that'll soon grow in complexity!)\n",
    "my_text = \"What day comes after Friday?\"\n",
    "my_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **èŠå¤©æ¶ˆæ¯**\n",
    "ç±»ä¼¼æ–‡æœ¬ï¼Œä½†æŒ‡å®šäº†æ¶ˆæ¯ç±»å‹ï¼ˆç³»ç»Ÿã€äººç±»ã€AIï¼‰\n",
    "\n",
    "* **ç³»ç»Ÿ** - æä¾›æœ‰ç”¨çš„èƒŒæ™¯ä¿¡æ¯ï¼Œå‘Šè¯‰AIåº”è¯¥åšä»€ä¹ˆ\n",
    "* **äººç±»** - ä»£è¡¨ç”¨æˆ·æ„å›¾çš„æ¶ˆæ¯\n",
    "* **AI** - æ˜¾ç¤ºAIçš„å“åº”æ¶ˆæ¯\n",
    "\n",
    "æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…OpenAIçš„[æ–‡æ¡£](https://platform.openai.com/docs/guides/chat/introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# This it the language model we'll use. We'll talk about what we're doing below in the next section\n",
    "chat = ChatOpenAI(temperature=.7, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨è®©æˆ‘ä»¬åˆ›å»ºä¸€äº›æ¶ˆæ¯ï¼Œæ¨¡æ‹Ÿä¸æœºå™¨äººçš„èŠå¤©ä½“éªŒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You could try a caprese salad with fresh tomatoes, mozzarella, and basil.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n",
    "        HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‚¨è¿˜å¯ä»¥ä¼ é€’æ›´å¤šçš„èŠå¤©å†å²è®°å½•ï¼ŒåŒ…æ‹¬AIçš„å“åº”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You should also explore the charming streets of the Old Town and indulge in delicious French cuisine.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
    "        HumanMessage(content=\"I like the beaches where should I go?\"),\n",
    "        AIMessage(content=\"You should go to Nice, France\"),\n",
    "        HumanMessage(content=\"What else should I do when I'm there?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœéœ€è¦ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ’é™¤ç³»ç»Ÿæ¶ˆæ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Friday')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        HumanMessage(content=\"What day comes after Thursday?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **æ–‡æ¡£**\n",
    "ä¸€ä¸ªåŒ…å«æ–‡æœ¬å’Œå…ƒæ•°æ®ï¼ˆå…³äºè¯¥æ–‡æœ¬çš„æ›´å¤šä¿¡æ¯ï¼‰çš„å¯¹è±¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\", metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"The LangChain Papers\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½†æ˜¯ï¼Œå¦‚æœä½ ä¸æƒ³åŒ…å«å…ƒæ•°æ®ï¼Œä¹Ÿå¯ä»¥ä¸åŒ…å«ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹ - è¿æ¥åˆ°AIå¤§è„‘çš„æ¥å£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **è¯­è¨€æ¨¡å‹**\n",
    "ä¸€ä¸ªå°†æ–‡æœ¬è¾“å…¥ â¡ï¸ æ–‡æœ¬è¾“å‡ºçš„æ¨¡å‹ï¼\n",
    "\n",
    "*çœ‹çœ‹æˆ‘å¦‚ä½•å°†æˆ‘ä½¿ç”¨çš„æ¨¡å‹ä»é»˜è®¤æ¨¡å‹æ›´æ”¹ä¸ºada-001ï¼ˆä¸€ä¸ªéå¸¸ä¾¿å®œï¼Œæ€§èƒ½ä½ä¸‹çš„æ¨¡å‹ï¼‰ã€‚åœ¨[è¿™é‡Œ](https://platform.openai.com/docs/models)æŸ¥çœ‹æ›´å¤šæ¨¡å‹*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-ada-001\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSaturday'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What day comes after Friday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **èŠå¤©æ¨¡å‹**\n",
    "ä¸€ä¸ªæ¥æ”¶ä¸€ç³»åˆ—æ¶ˆæ¯å¹¶è¿”å›æ¶ˆæ¯è¾“å‡ºçš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the math book go to New York? Because it had too many problems and needed a change of scenery!')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user says\"),\n",
    "        HumanMessage(content=\"I would like to go to New York, how should I do this?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‡½æ•°è°ƒç”¨æ¨¡å‹\n",
    "\n",
    "[å‡½æ•°è°ƒç”¨æ¨¡å‹](https://openai.com/blog/function-calling-and-other-api-updates)ä¸èŠå¤©æ¨¡å‹ç±»ä¼¼ï¼Œä½†æœ‰ä¸€äº›é¢å¤–çš„ç‰¹æ€§ã€‚å®ƒä»¬ç»è¿‡å¾®è°ƒï¼Œå¯ä»¥æä¾›ç»“æ„åŒ–çš„æ•°æ®è¾“å‡ºã€‚\n",
    "\n",
    "å½“ä½ éœ€è¦è°ƒç”¨å¤–éƒ¨æœåŠ¡çš„APIæˆ–è¿›è¡Œæ•°æ®æå–æ—¶ï¼Œè¿™å°†éå¸¸æ–¹ä¾¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}'}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(model='gpt-3.5-turbo-0613', temperature=1, openai_api_key=openai_api_key)\n",
    "\n",
    "output = chat(messages=\n",
    "     [\n",
    "         SystemMessage(content=\"You are an helpful AI bot\"),\n",
    "         HumanMessage(content=\"Whatâ€™s the weather like in Boston right now?\")\n",
    "     ],\n",
    "     functions=[{\n",
    "         \"name\": \"get_current_weather\",\n",
    "         \"description\": \"Get the current weather in a given location\",\n",
    "         \"parameters\": {\n",
    "             \"type\": \"object\",\n",
    "             \"properties\": {\n",
    "                 \"location\": {\n",
    "                     \"type\": \"string\",\n",
    "                     \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                 },\n",
    "                 \"unit\": {\n",
    "                     \"type\": \"string\",\n",
    "                     \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                 }\n",
    "             },\n",
    "             \"required\": [\"location\"]\n",
    "         }\n",
    "     }\n",
    "     ]\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çœ‹åˆ°ä¼ å›ç»™æˆ‘ä»¬çš„é¢å¤–çš„`additional_kwargs`äº†å—ï¼Ÿæˆ‘ä»¬å¯ä»¥æ‹¿åˆ°å®ƒå¹¶ä¼ é€’ç»™å¤–éƒ¨APIä»¥è·å–æ•°æ®ã€‚è¿™çœå»äº†è¿›è¡Œè¾“å‡ºè§£æçš„éº»çƒ¦ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **æ–‡æœ¬åµŒå…¥æ¨¡å‹**\n",
    "å°†ä½ çš„æ–‡æœ¬è½¬æ¢æˆå‘é‡ï¼ˆä¸€ç³»åˆ—ä¿æŒä½ çš„æ–‡æœ¬è¯­ä¹‰â€œå«ä¹‰â€çš„æ•°å­—ï¼‰ã€‚ä¸»è¦ç”¨äºæ¯”è¾ƒä¸¤æ®µæ–‡æœ¬ã€‚\n",
    "\n",
    "*é¡ºä¾¿è¯´ä¸€ä¸‹ï¼šè¯­ä¹‰æ„å‘³ç€â€œä¸è¯­è¨€æˆ–é€»è¾‘ä¸­çš„å«ä¹‰ç›¸å…³â€ã€‚*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi! It's time for the beach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a sample: [-0.00019600906371495047, -0.0031846734422911363, -0.0007734206914647714, -0.019472001962491232, -0.015092319017854244]...\n",
      "Your embedding is length 1536\n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings.embed_query(text)\n",
    "print (f\"Here's a sample: {text_embedding[:5]}...\")\n",
    "print (f\"Your embedding is length {len(text_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts - é€šå¸¸ç”¨ä½œæŒ‡å¯¼ä½ çš„æ¨¡å‹çš„æ–‡æœ¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompt**\n",
    "ä½ å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹çš„å†…å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The statement is incorrect. Tomorrow is Tuesday, not Wednesday.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "# I like to use three double quotation marks for my prompts because it's easier to read\n",
    "prompt = \"\"\"\n",
    "Today is Monday, tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with that statement?\n",
    "\"\"\"\n",
    "\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompt Template**\n",
    "ä¸€ä¸ªåŸºäºç”¨æˆ·è¾“å…¥ã€å…¶ä»–éé™æ€ä¿¡æ¯å’Œå›ºå®šæ¨¡æ¿å­—ç¬¦ä¸²çš„ç»„åˆï¼Œå¸®åŠ©åˆ›å»ºæç¤ºçš„å¯¹è±¡ã€‚\n",
    "\n",
    "å¯ä»¥å°†å…¶è§†ä¸ºPythonä¸­çš„[f-string](https://realpython.com/python-f-strings/)ï¼Œä½†ç”¨äºæç¤ºã€‚\n",
    "\n",
    "*é«˜çº§ï¼šæŸ¥çœ‹ [LangSmithHub](https://smith.langchain.com/hub) è·å–æ›´å¤šç¤¾åŒºæç¤ºæ¨¡æ¿*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "I really want to travel to Rome. What should I do there?\n",
      "\n",
      "Respond in one short sentence\n",
      "\n",
      "-----------\n",
      "LLM Output: Visit the Colosseum, the Vatican, and the Trevi Fountain.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Notice \"location\" below, that is a placeholder for another value later\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location='Rome')\n",
    "\n",
    "print (f\"Final Prompt: {final_prompt}\")\n",
    "print (\"-----------\")\n",
    "print (f\"LLM Output: {llm(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example Selectors**\n",
    "ä¸€ç§ä»ä¸€ç³»åˆ—ç¤ºä¾‹ä¸­é€‰æ‹©çš„ç®€å•æ–¹æ³•ï¼Œå…è®¸ä½ å°†ä¸Šä¸‹æ–‡ä¿¡æ¯åŠ¨æ€åœ°æ”¾å…¥ä½ çš„æç¤ºä¸­ã€‚å½“ä½ çš„ä»»åŠ¡å…·æœ‰ç»†å¾®å·®åˆ«æˆ–ä½ æœ‰ä¸€é•¿åˆ—ç¤ºä¾‹æ—¶ï¼Œè¿™ç§æ–¹æ³•ç»å¸¸è¢«ä½¿ç”¨ã€‚\n",
    "\n",
    "æŸ¥çœ‹ä¸åŒç±»å‹çš„ç¤ºä¾‹é€‰æ‹©å™¨[è¿™é‡Œ](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/)\n",
    "\n",
    "å¦‚æœä½ æƒ³äº†è§£ä¸ºä»€ä¹ˆç¤ºä¾‹å¾ˆé‡è¦ï¼ˆæç¤ºå·¥ç¨‹ï¼‰ï¼Œè¯·æŸ¥çœ‹[è¿™ä¸ªè§†é¢‘](https://www.youtube.com/watch?v=dOxUroR57xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregorykamradt/opt/anaconda3/lib/python3.9/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.7.2) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples, \n",
    "    \n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key), \n",
    "    \n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma, \n",
    "    \n",
    "    # This is the number of examples to produce.\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will help select examples\n",
    "    example_selector=example_selector,\n",
    "    \n",
    "    # Your prompt\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    # Customizations that will be added to the top and bottom of your prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    \n",
    "    # What inputs your prompt will receive\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the location an item is usually found in\n",
      "\n",
      "Example Input: tree\n",
      "Example Output: ground\n",
      "\n",
      "Example Input: bird\n",
      "Example Output: nest\n",
      "\n",
      "Input: plant\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Select a noun!\n",
    "my_noun = \"plant\"\n",
    "# my_noun = \"student\"\n",
    "\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' pot'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Output Parsers Method 1: Prompt Instructions & String Parsing**\n",
    "ä¸€ç§æ ¼å¼åŒ–æ¨¡å‹è¾“å‡ºçš„æœ‰ç”¨æ–¹æ³•ã€‚é€šå¸¸ç”¨äºç»“æ„åŒ–è¾“å‡ºã€‚LangChainåœ¨ä»–ä»¬çš„[æ–‡æ¡£](https://python.langchain.com/docs/modules/model_io/output_parsers)ä¸­åˆ—å‡ºäº†æ›´å¤šè¾“å‡ºè§£æå™¨ã€‚\n",
    "\n",
    "ä¸¤ä¸ªé‡è¦æ¦‚å¿µï¼š\n",
    "\n",
    "**1. æ ¼å¼æŒ‡ä»¤** - ä¸€ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„æç¤ºï¼Œå‘Šè¯‰LLMå¦‚ä½•æ ¹æ®ä½ æƒ³è¦çš„ç»“æœæ ¼å¼åŒ–å®ƒçš„å“åº”\n",
    "\n",
    "**2. è§£æå™¨** - ä¸€ç§æ–¹æ³•ï¼Œå°†æ¨¡å‹çš„æ–‡æœ¬è¾“å‡ºæå–åˆ°ä¸€ä¸ªæœŸæœ›çš„ç»“æ„ä¸­ï¼ˆé€šå¸¸æ˜¯jsonï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How you would like your response structured. This is basically a fancy prompt template\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# See the prompt template you created for formatting\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print (format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You will be given a poorly formatted string from a user.\n",
      "Reformat it and make sure all the words are spelled correctly\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT:\n",
      "welcom to califonya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"bad_string\": \"welcom to califonya!\", \\n\\t\"good_string\": \"Welcome to California!\"\\n}\\n```'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_output = llm(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'welcom to califonya!', 'good_string': 'Welcome to California!'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Output Parsers Method 2: OpenAI Functions**\n",
    "å½“OpenAIå‘å¸ƒå‡½æ•°è°ƒç”¨åŠŸèƒ½æ—¶ï¼Œæ¸¸æˆè§„åˆ™å‘ç”Ÿäº†å˜åŒ–ã€‚è¿™æ˜¯åˆšå¼€å§‹æ—¶æ¨èçš„æ–¹æ³•ã€‚\n",
    "\n",
    "ä»–ä»¬ä¸“é—¨è®­ç»ƒäº†æ¨¡å‹æ¥è¾“å‡ºç»“æ„åŒ–æ•°æ®ã€‚é€šè¿‡æŒ‡å®šPydanticæ¨¡å¼æ¥è·å–ç»“æ„åŒ–è¾“å‡ºå˜å¾—éå¸¸ç®€å•ã€‚\n",
    "\n",
    "å®šä¹‰æ¨¡å¼çš„æ–¹æ³•æœ‰å¾ˆå¤šï¼Œæˆ‘æ›´å–œæ¬¢ä½¿ç”¨Pydanticæ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬çš„ç»„ç»‡æ–¹å¼ã€‚å¯ä»¥å‚è€ƒOpenAIçš„[æ–‡æ¡£](https://platform.openai.com/docs/guides/gpt/function-calling)äº†è§£å…¶ä»–æ–¹æ³•ã€‚\n",
    "\n",
    "è¦ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œä½ éœ€è¦ä½¿ç”¨æ”¯æŒ[å‡½æ•°è°ƒç”¨](https://openai.com/blog/function-calling-and-other-api-updates#:~:text=Developers%20can%20now%20describe%20functions%20to%20gpt%2D4%2D0613%20and%20gpt%2D3.5%2Dturbo%2D0613%2C)çš„æ¨¡å‹ã€‚æˆ‘å°†ä½¿ç”¨`gpt4-0613`\n",
    "\n",
    "**ç¤ºä¾‹ 1: ç®€å•**\n",
    "\n",
    "è®©æˆ‘ä»¬å¼€å§‹å®šä¹‰ä¸€ä¸ªç®€å•çš„æ¨¡å‹æ¥æå–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Identifying information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The person's name\")\n",
    "    age: int = Field(..., description=\"The person's age\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"The person's favorite food\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç„¶åï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªé“¾ï¼ˆç¨åä¼šæœ‰æ›´å¤šä»‹ç»ï¼‰ï¼Œå®ƒå°†ä¸ºæˆ‘ä»¬å®Œæˆæå–å·¥ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Sally, Joey, Caroline', age=13, fav_food='spinach')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.openai_functions import create_structured_output_chain\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-0613', openai_api_key=openai_api_key)\n",
    "\n",
    "chain = create_structured_output_chain(Person, llm, prompt)\n",
    "chain.run(\n",
    "    \"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ³¨æ„æˆ‘ä»¬åªä»é‚£ä¸ªåˆ—è¡¨ä¸­å¾—åˆ°äº†ä¸€ä¸ªäººçš„æ•°æ®å—ï¼Ÿé‚£æ˜¯å› ä¸ºæˆ‘ä»¬æ²¡æœ‰æŒ‡å®šæˆ‘ä»¬æƒ³è¦å¤šä¸ªã€‚è®©æˆ‘ä»¬æ”¹å˜æˆ‘ä»¬çš„æ¨¡å¼ï¼Œä»¥æŒ‡å®šå¦‚æœå¯èƒ½çš„è¯ï¼Œæˆ‘ä»¬æƒ³è¦ä¸€ä¸ªäººå‘˜åˆ—è¡¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: Sequence[Person] = Field(..., description=\"The people in the text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨æˆ‘ä»¬å°†è°ƒç”¨Peopleè€Œä¸æ˜¯Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "People(people=[Person(name='Sally', age=13, fav_food=None), Person(name='Joey', age=12, fav_food='spinach'), Person(name='Caroline', age=23, fav_food=None)])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = create_structured_output_chain(People, llm, prompt)\n",
    "chain.run(\n",
    "    \"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®©æˆ‘ä»¬åšæ›´å¤šçš„è§£æ\n",
    "\n",
    "**ç¤ºä¾‹ 2: æšä¸¾**\n",
    "\n",
    "ç°åœ¨è®©æˆ‘ä»¬è§£æå½“æåˆ°åˆ—è¡¨ä¸­çš„ä¸€ä¸ªäº§å“æ—¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-0613', openai_api_key=openai_api_key)\n",
    "\n",
    "class Product(str, enum.Enum):\n",
    "    CRM = \"CRM\"\n",
    "    VIDEO_EDITING = \"VIDEO_EDITING\"\n",
    "    HARDWARE = \"HARDWARE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Products(BaseModel):\n",
    "    \"\"\"Identifying products that were mentioned in a text\"\"\"\n",
    "\n",
    "    products: Sequence[Product] = Field(..., description=\"The products mentioned in a text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Products(products=[<Product.CRM: 'CRM'>, <Product.HARDWARE: 'HARDWARE'>, <Product.VIDEO_EDITING: 'VIDEO_EDITING'>])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = create_structured_output_chain(Products, llm, prompt)\n",
    "chain.run(\n",
    "    \"The CRM in this demo is great. Love the hardware. The microphone is also cool. Love the video editing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç´¢å¼• - å°†æ–‡æ¡£ç»“æ„åŒ–ä»¥ä¾¿LLMå¯ä»¥ä½¿ç”¨å®ƒä»¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Document Loaders**\n",
    "ä»å…¶ä»–æ¥æºå¯¼å…¥æ•°æ®çš„ç®€ä¾¿æ–¹æ³•ã€‚ä¸[OpenAI æ’ä»¶](https://openai.com/blog/chatgpt-plugins)å…±äº«åŠŸèƒ½ï¼Œ[ç‰¹åˆ«æ˜¯æ£€ç´¢æ’ä»¶](https://github.com/openai/chatgpt-retrieval-plugin)ã€‚\n",
    "\n",
    "åœ¨è¿™é‡ŒæŸ¥çœ‹æ–‡æ¡£åŠ è½½å™¨çš„[å¤§åˆ—è¡¨](https://python.langchain.com/docs/integrations/document_loaders/)ã€‚[Llama Index](https://llamahub.ai/)ä¸Šè¿˜æœ‰æ›´å¤šã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HackerNews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 comments\n",
      "Here's a sample:\n",
      "\n",
      "Ozzie_osman 8 months ago  \n",
      "             | next [â€“] \n",
      "\n",
      "LangChain is awesome. For people not sure what it's doing, large language models (LLMs) are very Ozzie_osman 8 months ago  \n",
      "             | parent | next [â€“] \n",
      "\n",
      "Also, another library to check out is GPT Index (https://github.com/jerryjliu/gpt_index)\n"
     ]
    }
   ],
   "source": [
    "print (f\"Found {len(data)} comments\")\n",
    "print (f\"Here's a sample:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Books from Gutenberg Project**\n",
    "\n",
    "å›½å¤–å…è´¹ç”µå­ä¹¦å¹³å°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GutenbergLoader\n",
    "\n",
    "loader = GutenbergLoader(\"https://www.gutenberg.org/cache/epub/2148/pg2148.txt\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      At Paris, just after dark one gusty evening in the autumn of 18-,\n",
      "\n",
      "\n",
      "      I was enjoying the twofold luxury of meditation \n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content[1855:1984])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**URLs å’Œç½‘é¡µ**\n",
    "\n",
    "è®©æˆ‘ä»¬è¯•è¯•çœ‹ [Paul Graham çš„ç½‘ç«™](http://www.paulgraham.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New: \\n\\nHow to Do Great Work |\\nRead |\\nWill |\\nTruth\\n\\n\\n\\n\\n\\nWant to start a startup? Get funded by Y Combinator.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nÂ© mmxxiii pg'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "urls = [\n",
    "    \"http://www.paulgraham.com/\",\n",
    "]\n",
    "\n",
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Splitters**\n",
    "å¾ˆå¤šæ—¶å€™ä½ çš„æ–‡æ¡£å¤ªé•¿äº†ï¼ˆæ¯”å¦‚ä¸€æœ¬ä¹¦ï¼‰å¯¹äºä½ çš„LLMæ¥è¯´ã€‚ä½ éœ€è¦å°†å®ƒåˆ†å‰²æˆå—ã€‚æ–‡æœ¬åˆ†å‰²å™¨å¯ä»¥å¸®åŠ©åšåˆ°è¿™ä¸€ç‚¹ã€‚\n",
    "\n",
    "æœ‰è®¸å¤šæ–¹æ³•å¯ä»¥å°†ä½ çš„æ–‡æœ¬åˆ†å‰²æˆå—ï¼Œå°è¯•[ä¸åŒçš„æ–¹æ³•](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)æ¥çœ‹å“ªç§æœ€é€‚åˆä½ ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n"
     ]
    }
   ],
   "source": [
    "# This is a long document we can split up.\n",
    "with open('data/PaulGrahamEssays/worked.txt') as f:\n",
    "    pg_work = f.read()\n",
    "    \n",
    "print (f\"You have {len([pg_work])} document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 150,\n",
    "    chunk_overlap  = 20,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([pg_work])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 610 documents\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview:\n",
      "February 2021Before college the two main things I worked on, outside of school,\n",
      "were writing and programming. I didn't write essays. I wrote what \n",
      "\n",
      "beginning writers were supposed to write then, and probably still\n",
      "are: short stories. My stories were awful. They had hardly any plot,\n"
     ]
    }
   ],
   "source": [
    "print (\"Preview:\")\n",
    "print (texts[0].page_content, \"\\n\")\n",
    "print (texts[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ‰è®¸å¤šä¸åŒçš„æ–¹æ³•æ¥è¿›è¡Œæ–‡æœ¬åˆ†å‰²ï¼Œè¿™çœŸçš„å–å†³äºä½ çš„æ£€ç´¢ç­–ç•¥å’Œåº”ç”¨è®¾è®¡ã€‚åœ¨[è¿™é‡Œ](https://python.langchain.com/docs/modules/data_connection/document_transformers/)æŸ¥çœ‹æ›´å¤šåˆ†å‰²å™¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retrievers**\n",
    "å°†æ–‡æ¡£ä¸è¯­è¨€æ¨¡å‹ç»“åˆçš„ç®€ä¾¿æ–¹æ³•ã€‚\n",
    "\n",
    "æœ‰è®¸å¤šä¸åŒç±»å‹çš„æ£€ç´¢å™¨ï¼Œæœ€å¹¿æ³›æ”¯æŒçš„æ˜¯ VectoreStoreRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Embedd your texts\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init your retriever. Asking for just 1 document back\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x7f8389169070>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"what types of things did the author want to build?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standards; what was the point? No one else wanted one either, so\n",
      "off they went. That was what happened to systems work.I wanted not just to build things, but to build things that would\n",
      "last.In this di\n",
      "\n",
      "much of it in grad school.Computer Science is an uneasy alliance between two halves, theory\n",
      "and systems. The theory people prove things, and the systems people\n",
      "build things. I wanted to build things. \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **VectorStores**\n",
    "ç”¨äºå­˜å‚¨å‘é‡çš„æ•°æ®åº“ã€‚æœ€å—æ¬¢è¿çš„æœ‰ Mivusã€ [Pinecone](https://www.pinecone.io/) å’Œ [Weaviate](https://weaviate.io/)ã€‚åœ¨ OpenAI çš„[æ£€ç´¢æ–‡æ¡£](https://github.com/openai/chatgpt-retrieval-plugin#choosing-a-vector-database)ä¸Šæœ‰æ›´å¤šç¤ºä¾‹ã€‚[Chroma](https://www.trychroma.com/) å’Œ [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) åœ¨æœ¬åœ°å·¥ä½œèµ·æ¥å¾ˆæ–¹ä¾¿ã€‚\n",
    "\n",
    "ä»æ¦‚å¿µä¸Šè®²ï¼ŒæŠŠå®ƒä»¬æƒ³è±¡æˆå¸¦æœ‰ä¸€ä¸ªåµŒå…¥ï¼ˆå‘é‡ï¼‰åˆ—å’Œä¸€ä¸ªå…ƒæ•°æ®åˆ—çš„è¡¨æ ¼ã€‚\n",
    "\n",
    "ç¤ºä¾‹\n",
    "\n",
    "| åµŒå…¥          | å…ƒæ•°æ®       |\n",
    "| ----------- | ----------- |\n",
    "| [-0.00015641732898075134, -0.003165106289088726, ...]      | {'date' : '1/2/23}       |\n",
    "| [-0.00035465431654651654, 1.4654131651654516546, ...]   | {'date' : '1/3/23}        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 78 documents\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = embeddings.embed_documents([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 78 embeddings\n",
      "Here's a sample of one: [-0.001058628615053026, -0.01118234211553424, -0.012874804746266883]...\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(embedding_list)} embeddings\")\n",
    "print (f\"Here's a sample of one: {embedding_list[0][:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ çš„å‘é‡å­˜å‚¨åº“å­˜å‚¨ä½ çš„åµŒå…¥ï¼ˆâ˜ï¸ï¼‰å¹¶ä½¿å®ƒä»¬æ˜“äºæœç´¢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "å¸®åŠ©LLMè®°ä½ä¿¡æ¯ã€‚\n",
    "\n",
    "Memoryæ˜¯ä¸€ä¸ªç›¸å¯¹å®½æ³›çš„æœ¯è¯­ã€‚å®ƒå¯ä»¥ç®€å•åˆ°è®°ä½ä½ è¿‡å»èŠè¿‡çš„ä¿¡æ¯ï¼Œæˆ–è€…æ˜¯æ›´å¤æ‚çš„ä¿¡æ¯æ£€ç´¢ã€‚\n",
    "\n",
    "æˆ‘ä»¬å°†å…¶åº”ç”¨äºèŠå¤©æ¶ˆæ¯ç”¨ä¾‹ã€‚è¿™å°†ç”¨äºèŠå¤©æœºå™¨äººã€‚\n",
    "\n",
    "æœ‰è®¸å¤šç±»å‹çš„è®°å¿†ï¼Œæ¢ç´¢[æ–‡æ¡£](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html)æ¥çœ‹å“ªä¸€ä¸ªé€‚åˆä½ çš„ç”¨ä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_ai_message(\"hi!\")\n",
    "\n",
    "history.add_user_message(\"what is the capital of france?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!'),\n",
       " HumanMessage(content='what is the capital of france?')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_response = chat(history.messages)\n",
    "ai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!'),\n",
       " HumanMessage(content='what is the capital of france?'),\n",
       " AIMessage(content='The capital of France is Paris.')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.add_ai_message(ai_response.content)\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains â›“ï¸â›“ï¸â›“ï¸\n",
    "è‡ªåŠ¨ç»“åˆä¸åŒçš„LLMè°ƒç”¨å’Œè¡ŒåŠ¨\n",
    "\n",
    "ä¾‹å¦‚ï¼šæ‘˜è¦ #1, æ‘˜è¦ #2, æ‘˜è¦ #3 > æœ€ç»ˆæ‘˜è¦\n",
    "\n",
    "æŸ¥çœ‹[è¿™ä¸ªè§†é¢‘](https://www.youtube.com/watch?v=f9_BWhCI4Zo&t=2s)è§£é‡Šä¸åŒçš„æ‘˜è¦é“¾ç±»å‹\n",
    "\n",
    "æœ‰è®¸å¤š[é“¾çš„åº”ç”¨](https://python.langchain.com/docs/modules/chains/)æœç´¢çœ‹çœ‹å“ªäº›æœ€é€‚åˆä½ çš„ç”¨ä¾‹ã€‚\n",
    "\n",
    "æˆ‘ä»¬å°†ä»‹ç»å…¶ä¸­çš„ä¸¤ä¸ªï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple Sequential Chains\n",
    "\n",
    "ç®€å•çš„é“¾æ¡ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸€ä¸ªLLMçš„è¾“å‡ºä½œä¸ºå¦ä¸€ä¸ªLLMçš„è¾“å…¥ã€‚é€‚åˆæ‹†åˆ†ä»»åŠ¡ï¼ˆå¹¶ä¿æŒä½ çš„LLMä¸“æ³¨ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = OpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Holds my 'location' chain\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Holds my 'meal' chain\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "A classic dish from Rome is Spaghetti alla Carbonara, featuring egg, Parmesan cheese, black pepper, and pancetta or guanciale.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Ingredients:\n",
      "- 8oz spaghetti \n",
      "- 4 tablespoons olive oil\n",
      "- 4oz diced pancetta or guanciale\n",
      "- 2 cloves garlic, minced\n",
      "- 2 eggs, lightly beaten\n",
      "- 2 tablespoons parsley, chopped \n",
      "- Â½ cup grated Parmesan \n",
      "- Salt and black pepper to taste\n",
      "\n",
      "Instructions:\n",
      "1. Bring a pot of salted water to a boil and add the spaghetti. Cook according to package directions. \n",
      "2. Meanwhile, add the olive oil to a large skillet over medium-high heat. Add the diced pancetta and garlic, and cook until pancetta is browned and garlic is fragrant.\n",
      "3. In a medium bowl, whisk together the eggs, parsley, Parmesan, and salt and pepper.\n",
      "4. Drain the cooked spaghetti and add it to the skillet with the pancetta and garlic. Remove from heat and pour the egg mixture over the spaghetti, stirring to combine. \n",
      "5. Serve the spaghetti alla carbonara with additional Parmesan cheese and black pepper.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "review = overall_chain.run(\"Rome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summarization Chain\n",
    "\n",
    "è½»æ¾åœ°æµè§ˆå¤§é‡é•¿æ–‡æ¡£å¹¶è·å–æ‘˜è¦ã€‚é™¤äº† map-reduce ä¹‹å¤–ï¼Œè¿˜å¯ä»¥æŸ¥çœ‹[è¿™ä¸ªè§†é¢‘](https://www.youtube.com/watch?v=f9_BWhCI4Zo)äº†è§£å…¶ä»–é“¾ç±»å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"January 2017Because biographies of famous scientists tend to \n",
      "edit out their mistakes, we underestimate the \n",
      "degree of risk they were willing to take.\n",
      "And because anything a famous scientist did that\n",
      "wasn't a mistake has probably now become the\n",
      "conventional wisdom, those choices don't\n",
      "seem risky either.Biographies of Newton, for example, understandably focus\n",
      "more on physics than alchemy or theology.\n",
      "The impression we get is that his unerring judgment\n",
      "led him straight to truths no one else had noticed.\n",
      "How to explain all the time he spent on alchemy\n",
      "and theology?  Well, smart people are often kind of\n",
      "crazy.But maybe there is a simpler explanation. Maybe\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"the smartness and the craziness were not as separate\n",
      "as we think. Physics seems to us a promising thing\n",
      "to work on, and alchemy and theology obvious wastes\n",
      "of time. But that's because we know how things\n",
      "turned out. In Newton's day the three problems \n",
      "seemed roughly equally promising. No one knew yet\n",
      "what the payoff would be for inventing what we\n",
      "now call physics; if they had, more people would \n",
      "have been working on it. And alchemy and theology\n",
      "were still then in the category Marc Andreessen would \n",
      "describe as \"huge, if true.\"Newton made three bets. One of them worked. But \n",
      "they were all risky.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" Biographies of famous scientists often edit out their mistakes, giving readers the wrong impression that they never faced any risks to achieve successful results. An example of this is Newton, whose smartness is assumed to have led straight him to truths without any detours into alchemy or theology - despite the fact that he spent a lot of time on both fields. Maybe the simpler explanation is that he was willing to take risks, even if it means potentially making mistakes.\n",
      "\n",
      " In the 17th century, Newton took a risk and made three bets, one of which turned out to be a successful invention of what we now call physics. The other two bets were on less popular subjects of the time such as alchemy and theology. People did not know then what the payoff would be, but the bets still seemed relatively promising.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Biographies tend to omit famous scientists' mistakes from their stories, but Newton was willing to take risks and explore multiple fields to make his discoveries. He placed three risky bets, one of which resulted in the creation of physics as we know it today.\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/disc.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# There is a lot of complexity hidden in this one line. I encourage you to check out the video above for more detail\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents ğŸ¤–ğŸ¤–\n",
    "\n",
    "å®˜æ–¹LangChainæ–‡æ¡£å®Œç¾åœ°æè¿°äº†ä»£ç†ï¼š\n",
    "> ä¸€äº›åº”ç”¨ç¨‹åºä¸ä»…éœ€è¦é¢„å®šçš„LLM/å…¶ä»–å·¥å…·è°ƒç”¨é“¾ï¼Œè€Œä¸”å¯èƒ½éœ€è¦ä¸€ä¸ª**æœªçŸ¥çš„é“¾**ï¼Œè¿™å–å†³äºç”¨æˆ·çš„è¾“å…¥ã€‚åœ¨è¿™äº›ç±»å‹çš„é“¾ä¸­ï¼Œæœ‰ä¸€ä¸ªâ€œä»£ç†â€ï¼Œå®ƒå¯ä»¥è®¿é—®ä¸€å¥—å·¥å…·ã€‚æ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œä»£ç†å¯ä»¥**å†³å®šè°ƒç”¨å“ªäº›å·¥å…·ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰**ã€‚\n",
    "\n",
    "åŸºæœ¬ä¸Šï¼Œä½ ä½¿ç”¨LLMä¸ä»…ä»…æ˜¯ä¸ºäº†æ–‡æœ¬è¾“å‡ºï¼Œè€Œä¸”è¿˜ç”¨äºå†³ç­–ã€‚è¿™ä¸ªåŠŸèƒ½çš„é…·ç‚«å’Œå¼ºå¤§æ˜¯æ— æ³•è¿‡åˆ†å¼ºè°ƒçš„ã€‚\n",
    "\n",
    "Sam Altmanå¼ºè°ƒLLMæ˜¯å¾ˆå¥½çš„'[æ¨ç†å¼•æ“](https://www.youtube.com/watch?v=L_Guz73e6fw&t=867s)'ã€‚ä»£ç†åˆ©ç”¨äº†è¿™ä¸€ç‚¹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "ä»£ç†çš„â€œèƒ½åŠ›â€ã€‚è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½ä¹‹ä¸Šçš„æŠ½è±¡ï¼Œä½¿LLMï¼ˆå’Œä»£ç†ï¼‰æ˜“äºä¸ä¹‹äº¤äº’ã€‚ä¾‹å¦‚ï¼šGoogleæœç´¢ã€‚\n",
    "\n",
    "è¿™ä¸ªé¢†åŸŸä¸[OpenAIæ’ä»¶](https://platform.openai.com/docs/plugins/introduction)æœ‰å…±é€šä¹‹å¤„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toolkit\n",
    "\n",
    "ä»£ç†å¯ä»¥é€‰æ‹©çš„å·¥å…·ç»„\n",
    "\n",
    "è®©æˆ‘ä»¬æŠŠå®ƒä»¬å…¨éƒ¨æ•´åˆèµ·æ¥ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "serpapi_api_key=os.getenv(\"SERP_API_KEY\", \"YourAPIKey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = load_tools([\"serpapi\"], llm=llm, serpapi_api_key=serpapi_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should try to find out what band Natalie Bergman is a part of.\n",
      "Action: Search\n",
      "Action Input: \"Natalie Bergman band\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Natalie Bergman is an American singer-songwriter. She is one half of the duo Wild Belle, along with her brother Elliot Bergman. Her debut solo album, Mercy, was released on Third Man Records on May 7, 2021. She is based in Los Angeles.', 'Natalie Bergman type: American singer-songwriter.', 'Natalie Bergman main_tab_text: Overview.', 'Natalie Bergman kgmid: /m/0qgx4kh.', 'Natalie Bergman genre: Folk.', 'Natalie Bergman parents: Susan Bergman, Judson Bergman.', 'Natalie Bergman born: 1988 or 1989 (age 34â€“35).', 'Natalie Bergman is an American singer-songwriter. She is one half of the duo Wild Belle, along with her brother Elliot Bergman. Her debut solo album, Mercy, ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should search for the first album of Wild Belle\n",
      "Action: Search\n",
      "Action Input: \"Wild Belle first album\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIsles\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Isles is the first album of the band that Natalie Bergman is a part of.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent({\"input\":\"what was the first album of the\" \n",
    "                    \"band that Natalie Bergman is a part of?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Wild Belle](../res/WildBelle1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸµEnjoyğŸµ\n",
    "https://open.spotify.com/track/1eREJIBdqeCcqNCB1pbz7w?si=c014293b63c7478c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
