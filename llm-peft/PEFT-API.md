



- prepare_model_for_int8_training 用途(deprecated, 直接使用 prepare_model_for_kbit_training)：

```
将所有非 int8 模块转换为全精度 (fp32) 以确保稳定性

在输入嵌入层添加一个前向钩子（forward hook）来计算输入隐藏状态的梯度

启用梯度检查点以提高内存效率的训练

```


---


- prepare_model_for_kbit_training 用途：

```
处理量化模型以用于训练。

1-转换 layernorm 为 fp32
2-使输出嵌入层计算梯度
3-转换lm head 为 fp32

```

参数：

- use_gradient_checkpointing：如果为 True，则使用梯度检查点来节省内存，但代价是向后传递速度较慢。

本质是减少内存消耗的一种方式，以时间或者计算换内存

gradient_checkpointing（梯度检查点）是一种用于减少深度学习模型中内存消耗的技术。在训练深度神经网络时，反向传播算法需要在前向传播和反向传播之间存储中间计算结果，以便计算梯度并更新模型参数。这些中间结果的存储会占用大量的内存，特别是当模型非常深或参数量很大时。

梯度检查点技术通过在前向传播期间临时丢弃一些中间结果，仅保留必要的信息，以减少内存使用量。在反向传播过程中，只需要重新计算被丢弃的中间结果，而不需要存储所有的中间结果，从而节省内存空间。

实现梯度检查点的一种常见方法是将某些层或操作标记为检查点。在前向传播期间，被标记为检查点的层将计算并缓存中间结果。然后，在反向传播过程中，这些层将重新计算其所需的中间结果，以便计算梯度。

具体来说，在完整的正向传播中，函数将以 torch.no_grad() 方式运行，即不存储中间激活。同时，在完整正向传播过程中，每遇到检查点则保存输入tensor和函数参数。等到反向传播时，把每个检查点看成一个局部阶段，在局部阶段中取出保存的输入tensor和函数参数，并再次计算局部阶段内函数的前向传播过程，跟踪每个神经元的激活值，然后使用这些激活值计算梯度。



---



